{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84b8a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. SETUP AWAL\n",
    "# Kita hanya panggil \"Kamus\" (Tokenizer) nya saja, bukan Model AI-nya.\n",
    "checkpoint = \"prajjwal1/bert-tiny\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Load Dataset\n",
    "emot = load_dataset(\"indonlp/indonlu\", \"emot\")\n",
    "smsa = load_dataset(\"indonlp/indonlu\", \"smsa\")\n",
    "nerp = load_dataset(\"indonlp/indonlu\", \"nerp\")\n",
    "\n",
    "# ======================================================\n",
    "# FUNGSI 1: Untuk Klasifikasi (SMSA & EMOT)\n",
    "# ======================================================\n",
    "def process_classification(examples):\n",
    "    # Deteksi nama kolom teks (karena beda-beda tiap dataset)\n",
    "    text_col = \"tweet\" if \"tweet\" in examples else \"text\"\n",
    "    \n",
    "    # Ubah Teks -> Angka (Input IDs)\n",
    "    tokenized = tokenizer(\n",
    "        examples[text_col], \n",
    "        truncation=True, \n",
    "        max_length=128,      # Batasi panjang kalimat\n",
    "        padding=\"max_length\" # Samakan panjang semua kalimat\n",
    "    )\n",
    "    \n",
    "    # Ambil label angka (0, 1, 2) dan simpan di kolom 'labels'\n",
    "    # Penting: Jangan diubah jadi string! Biarkan integer.\n",
    "    tokenized[\"labels\"] = examples[\"label\"]\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# ======================================================\n",
    "# FUNGSI 2: Untuk NER (NERP) - Lebih Rumit\n",
    "# ======================================================\n",
    "def process_ner(examples):\n",
    "    # Tokenisasi list kata (karena NER inputnya per kata)\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], \n",
    "        truncation=True, \n",
    "        is_split_into_words=True, # Wajib True!\n",
    "        max_length=128,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    # Loop setiap kalimat\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i) # Mapping token ke kata asli\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            # Jika token spesial ([CLS], [SEP], [PAD]) -> label -100 (diabaikan)\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # Jika token kata baru -> ambil label aslinya\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # Jika token pecahan kata (subword) -> label -100\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "            \n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# ======================================================\n",
    "# EKSEKUSI (Jalankan Preprocessing)\n",
    "# ======================================================\n",
    "\n",
    "print(\"Sedang memproses SMSA...\")\n",
    "smsa_ready = smsa.map(\n",
    "    process_classification, \n",
    "    batched=True, \n",
    "    remove_columns=smsa[\"train\"].column_names # Hapus kolom mentah\n",
    ")\n",
    "\n",
    "print(\"Sedang memproses EMOT...\")\n",
    "emot_ready = emot.map(\n",
    "    process_classification, \n",
    "    batched=True, \n",
    "    remove_columns=emot[\"train\"].column_names\n",
    ")\n",
    "\n",
    "print(\"Sedang memproses NERP (NER)...\")\n",
    "nerp_ready = nerp.map(\n",
    "    process_ner, \n",
    "    batched=True, \n",
    "    remove_columns=nerp[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# ======================================================\n",
    "# HASIL AKHIR\n",
    "# ======================================================\n",
    "print(\"\\nSukses! Data sudah bersih.\")\n",
    "print(\"Contoh data SMSA (siap training):\", smsa_ready[\"train\"][0].keys())\n",
    "# Output keys harus: ['input_ids', 'token_type_ids', 'attention_mask', 'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cek 1 Sampel dari SMSA\n",
    "sample = smsa_ready[\"train\"][0]\n",
    "print(\"=== CEK SMSA ===\")\n",
    "print(\"Input IDs:\", sample[\"input_ids\"][:10]) # Lihat 10 angka pertama\n",
    "print(\"Label ID :\", sample[\"labels\"])\n",
    "\n",
    "# Kembalikan ke teks asli untuk memastikan isinya bukan sampah\n",
    "decoded_text = tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=True)\n",
    "print(\"Decoded Text:\", decoded_text)\n",
    "\n",
    "\n",
    "# Cek 1 Sampel dari NER (PENTING! Cek angka -100 nya)\n",
    "sample_ner = nerp_ready[\"train\"][0]\n",
    "print(\"\\n=== CEK NER ===\")\n",
    "print(\"Input IDs:\", sample_ner[\"input_ids\"][:10])\n",
    "print(\"Labels   :\", sample_ner[\"labels\"][:10]) \n",
    "# Kamu harus melihat angka -100 di antara label lain jika ada subword!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90465d79",
   "metadata": {},
   "source": [
    "Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ec19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Buat folder penampung\n",
    "os.makedirs(\"processed_data\", exist_ok=True)\n",
    "\n",
    "# Simpan\n",
    "smsa_ready.save_to_disk(\"processed_data/smsa_processed\")\n",
    "emot_ready.save_to_disk(\"processed_data/emot_processed\")\n",
    "nerp_ready.save_to_disk(\"processed_data/nerp_processed\")\n",
    "\n",
    "print(\"\\nSemua data berhasil disimpan ke folder 'processed_data/'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a06799",
   "metadata": {},
   "source": [
    "Preprocessing QA (SQuAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10086fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Lokasi file kamu\n",
    "file_path = \"/kaggle/input/uad-id/train-SQuAD-id.json\"\n",
    "\n",
    "def load_and_flatten_squad(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        squad_dict = json.load(f)\n",
    "    \n",
    "    flattened_data = []\n",
    "    \n",
    "    # Masuk ke struktur 'data' -> 'paragraphs' -> 'qas'\n",
    "    for article in squad_dict[\"data\"]:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            \n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question = qa[\"question\"]\n",
    "                id_ = qa[\"id\"]\n",
    "                answers = qa[\"answers\"]\n",
    "                \n",
    "                # Format ulang jawaban agar sesuai standar Hugging Face\n",
    "                # Biasanya di JSON SQuAD formatnya: [{'text': '..', 'answer_start': 12}]\n",
    "                # Kita butuh format: {'text': ['..'], 'answer_start': [12]}\n",
    "                formatted_answers = {\n",
    "                    \"text\": [ans[\"text\"] for ans in answers],\n",
    "                    \"answer_start\": [ans[\"answer_start\"] for ans in answers]\n",
    "                }\n",
    "                \n",
    "                flattened_data.append({\n",
    "                    \"id\": id_,\n",
    "                    \"context\": context,\n",
    "                    \"question\": question,\n",
    "                    \"answers\": formatted_answers\n",
    "                })\n",
    "    \n",
    "    # Ubah list dictionary menjadi Hugging Face Dataset\n",
    "    return Dataset.from_list(flattened_data)\n",
    "\n",
    "# --- EKSEKUSI ---\n",
    "print(\"Sedang meratakan struktur JSON SQuAD...\")\n",
    "dataset_flat = load_and_flatten_squad(file_path)\n",
    "\n",
    "print(\"Sukses!\")\n",
    "print(\"Contoh data:\", dataset_flat[0])\n",
    "# Sekarang harusnya muncul keys: ['id', 'context', 'question', 'answers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851fb1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Pastikan Tokenizer sudah load\n",
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# 2. DEFINISI FUNGSI PREPROCESS_QA (VERSI ANTI-CRASH)\n",
    "def preprocess_qa(examples):\n",
    "    max_length = 384\n",
    "    doc_stride = 128\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        [q.strip() for q in examples[\"question\"]],\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        \n",
    "        # --- PERBAIKAN DI SINI (SABUK PENGAMAN) ---\n",
    "        # Cek apakah list jawaban kosong?\n",
    "        if len(answer[\"answer_start\"]) == 0:\n",
    "            # Jika kosong, arahkan ke CLS (index 0)\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            continue # Lanjut ke data berikutnya\n",
    "        # ------------------------------------------\n",
    "\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        \n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        # Cari batas index token konteks\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "# 3. EKSEKUSI ULANG\n",
    "print(\"Mencoba Preprocessing lagi (Versi Aman)...\")\n",
    "qa_processed = dataset_flat.map(\n",
    "    preprocess_qa,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_flat.column_names\n",
    ")\n",
    "\n",
    "print(\"✅ SUKSES! Preprocessing QA Selesai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f60edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"processed_data\", exist_ok=True)\n",
    "\n",
    "# Simpan SQuAD\n",
    "qa_processed.save_to_disk(\"processed_data/squad_processed\")\n",
    "\n",
    "print(\"Data SQuAD aman tersimpan! ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348301ea",
   "metadata": {},
   "source": [
    "Preprocessing WikiLingua (Khusus IndoT5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4640760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- PERUBAHAN DI SINI ---\n",
    "# Ganti dari 'indobenchmark/indot5-base' ke 'google/mt5-small'\n",
    "# mt5-small adalah versi ringan, kalau GPU kuat bisa pakai 'google/mt5-base'\n",
    "checkpoint_t5 = \"google/mt5-small\" \n",
    "\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(checkpoint_t5)\n",
    "\n",
    "# Load Data (tidak perlu diubah)\n",
    "wiki_lingua_id = load_dataset(\"wiki_lingua\", \"indonesian\", ignore_verifications=True)\n",
    "\n",
    "# Konfigurasi\n",
    "prefix = \"ringkas: \" \n",
    "max_input_length = 512\n",
    "max_target_length = 150\n",
    "\n",
    "def preprocess_summarization_t5(examples):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    for article_dict in examples[\"article\"]:\n",
    "        doc_text = \" \".join(article_dict[\"document\"]) \n",
    "        sum_text = \" \".join(article_dict[\"summary\"])\n",
    "        \n",
    "        inputs.append(prefix + doc_text)\n",
    "        targets.append(sum_text)\n",
    "    \n",
    "    model_inputs = tokenizer_t5(\n",
    "        inputs, \n",
    "        max_length=max_input_length, \n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    with tokenizer_t5.as_target_tokenizer():\n",
    "        labels = tokenizer_t5(\n",
    "            targets, \n",
    "            max_length=max_target_length, \n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(l if l != tokenizer_t5.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# EKSEKUSI\n",
    "print(\"Sedang memproses WikiLingua dengan Google mT5...\")\n",
    "wiki_processed = wiki_lingua_id[\"train\"].map(\n",
    "    preprocess_summarization_t5, \n",
    "    batched=True, \n",
    "    remove_columns=wiki_lingua_id[\"train\"].column_names\n",
    ")\n",
    "\n",
    "print(\"\\nSukses!\")\n",
    "print(\"Fitur:\", wiki_processed.features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd1849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"processed_data\", exist_ok=True)\n",
    "\n",
    "# Ganti nama folder jadi 'mt5' agar sesuai dengan tokenizer yang dipakai\n",
    "wiki_processed.save_to_disk(\"processed_data/wikilingua_mt5_processed\")\n",
    "\n",
    "print(\"Data Summary (mT5) aman!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
