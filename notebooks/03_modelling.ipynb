{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae96eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, AdaLoraConfig, TaskType\n",
    "from datasets import load_from_disk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# 1. SETUP & LOAD DATA\n",
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "dataset = load_from_disk(\"processed_data/smsa_processed\") \n",
    "\n",
    "id2label = {0: \"positif\", 1: \"netral\", 2: \"negatif\"}\n",
    "label2id = {\"positif\": 0, \"netral\": 1, \"negatif\": 2}\n",
    "\n",
    "# 2. LOAD BASE MODEL\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, \n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# --- REVISI HYPERPARAMETER (ANTI-OVERFIT) ---\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30  # Turunkan drastis dari 100 ke 30 (Kualitas > Kuantitas)\n",
    "train_samples = len(dataset[\"train\"])\n",
    "\n",
    "# Hitung Total Steps Baru\n",
    "total_steps = (train_samples // BATCH_SIZE) * NUM_EPOCHS\n",
    "\n",
    "# Hitung tinit dan tfinal\n",
    "t_init_steps = int(total_steps * 0.15) \n",
    "t_final_steps = int(total_steps * 0.20) \n",
    "\n",
    "print(f\"Total Steps Baru: {total_steps}\")\n",
    "print(f\"Jadwal AdaLoRA -> Init: {t_init_steps}, Final: {t_final_steps}\")\n",
    "\n",
    "# 3. KONFIGURASI ADALORA (LEBIH PINTAR & STABIL)\n",
    "peft_config = AdaLoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, \n",
    "    r=32,            # NAIKKAN: Biar kapasitas otaknya lebih besar\n",
    "    lora_alpha=64,   # NAIKKAN: Biasanya 2x dari r\n",
    "    target_modules=[\"query\", \"value\"], \n",
    "    lora_dropout=0.1, # NAIKKAN: Biar gak gampang menghafal (Overfitting)\n",
    "    bias=\"none\",\n",
    "    init_r=12, \n",
    "    target_r=8, \n",
    "    beta1=0.85, \n",
    "    beta2=0.85,\n",
    "    tinit=t_init_steps, \n",
    "    tfinal=t_final_steps, \n",
    "    deltaT=10,\n",
    "    total_step=total_steps \n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 4. METRIC\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# 5. TRAINING ARGUMENTS (OPTIMAL)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model_output/smsa_adalora_tuned\",\n",
    "    learning_rate=5e-4,      # TURUNKAN: Biar belajarnya pelan tapi pasti\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Strategi Evaluasi\n",
    "    eval_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    # --- KUNCI ANTI SAKIT HATI ---\n",
    "    metric_for_best_model=\"accuracy\", # Simpan berdasarkan Akurasi\n",
    "    greater_is_better=True,           # Semakin tinggi akurasi, semakin bagus\n",
    "    save_total_limit=2,               # Cuma simpan 2 model terbaik biar hemat storage\n",
    "    # -----------------------------\n",
    "    \n",
    "    logging_steps=50,   # Ubah ke 50 biar log-nya gak spamming (tapi tetep update)\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Inisialisasi Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=AutoTokenizer.from_pretrained(checkpoint),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Mulai Training Revisi (30 Epoch, Rank 32)...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298bb053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Ambil Data Log dari Trainer\n",
    "history = trainer.state.log_history\n",
    "df = pd.DataFrame(history)\n",
    "\n",
    "# 2. Pisahkan Data Training dan Data Validasi\n",
    "# Training log biasanya punya key 'loss', Validation punya 'eval_loss'\n",
    "train_loss = df[df['loss'].notna()][['epoch', 'loss']]\n",
    "val_loss = df[df['eval_loss'].notna()][['epoch', 'eval_loss']]\n",
    "val_acc = df[df['eval_accuracy'].notna()][['epoch', 'eval_accuracy']]\n",
    "\n",
    "# 3. Plotting\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# --- Grafik 1: Loss (Training vs Validation) ---\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss['epoch'], train_loss['loss'], label='Training Loss', color='blue', linestyle='--')\n",
    "plt.plot(val_loss['epoch'], val_loss['eval_loss'], label='Validation Loss', color='red', linewidth=2)\n",
    "plt.title('Kurva Loss (Semakin Rendah Semakin Bagus)', fontsize=14)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# --- Grafik 2: Akurasi (Validation) ---\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_acc['epoch'], val_acc['eval_accuracy'], label='Validation Accuracy', color='green', marker='o')\n",
    "plt.title('Kurva Akurasi (Semakin Tinggi Semakin Bagus)', fontsize=14)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9410a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\n",
    "from peft import PeftModel, PeftConfig\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP & LOAD MODEL YANG SUDAH JADI\n",
    "# ==========================================\n",
    "print(\"üìÇ Sedang meload Model SMSA dari: saved_models/smsa_adalora_best ...\")\n",
    "\n",
    "# A. Load Data SMSA\n",
    "dataset = load_from_disk(\"processed_data/smsa_processed\") \n",
    "\n",
    "# B. Label Mapping (Harus 3 kelas: Positif, Netral, Negatif)\n",
    "id2label = {0: \"positif\", 1: \"netral\", 2: \"negatif\"}\n",
    "label2id = {\"positif\": 0, \"netral\": 1, \"negatif\": 2}\n",
    "label_names = [\"Positif\", \"Netral\", \"Negatif\"]\n",
    "\n",
    "# C. Load Base Model (TinyBERT Polos)\n",
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, \n",
    "    num_labels=3, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# D. Load \"Otak\" AdaLoRA yang sudah kamu simpan\n",
    "model_path = \"saved_models/smsa_adalora_best\"\n",
    "\n",
    "try:\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    print(\"‚úÖ Model berhasil diload! Siap untuk ujian.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Gagal load model di path: {model_path}\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Pastikan nama foldernya benar-benar ada.\")\n",
    "    # Stop eksekusi jika gagal load\n",
    "    raise e\n",
    "\n",
    "# ==========================================\n",
    "# 2. INFERENCE MANUAL (ANTI-CRASH ADALORA)\n",
    "# ==========================================\n",
    "print(\"\\nüîç Sedang menjalankan Prediksi (Manual Loop)...\")\n",
    "\n",
    "# Setup Device (GPU/CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval() # Mode Evaluasi (Penting!)\n",
    "\n",
    "# Setup DataLoader\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "test_loader = DataLoader(dataset[\"test\"], batch_size=32, collate_fn=data_collator)\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Loop Prediksi\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        # Pindah ke GPU\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        labels = batch.pop(\"labels\")\n",
    "        \n",
    "        # Forward Pass\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        # Ambil prediksi (index dengan nilai tertinggi)\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# ==========================================\n",
    "# 3. VISUALISASI HASIL\n",
    "# ==========================================\n",
    "print(\"\\nüìä Membuat Visualisasi...\")\n",
    "\n",
    "# A. Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=label_names, yticklabels=label_names,\n",
    "            cbar_kws={'label': 'Jumlah Sampel'})\n",
    "\n",
    "plt.title('Confusion Matrix: SMSA (Sentiment Analysis)', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Prediksi Model', fontsize=12)\n",
    "plt.ylabel('Label Sebenarnya', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# B. Laporan Klasifikasi\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"HASIL AKHIR SMSA (DARI SAVED MODEL)\")\n",
    "print(\"=\"*50)\n",
    "report = classification_report(all_labels, all_preds, target_names=label_names, digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9350b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Saya ubah nama foldernya jadi 'best' biar lebih jelas\n",
    "save_path = \"saved_models/smsa_adalora_best\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# 1. Simpan Model\n",
    "# Karena load_best_model_at_end=True, ini OTOMATIS menyimpan checkpoint terbaik\n",
    "trainer.save_model(save_path)\n",
    "\n",
    "# 2. Simpan Tokenizer (Wajib sepaket)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"‚úÖ Best Model sukses disimpan di: {save_path}\")\n",
    "\n",
    "# 3. Cek Checkpoint Mana yang Dipilih\n",
    "# Ini untuk memastikan dia tidak mengambil epoch 100\n",
    "if trainer.state.best_model_checkpoint:\n",
    "    print(f\"‚ÑπÔ∏è Model ini diambil dari checkpoint: {trainer.state.best_model_checkpoint}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Info checkpoint tidak tersedia, tapi model yang tersimpan tetap yang terbaik.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519e7af4",
   "metadata": {},
   "source": [
    "Emot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, AdaLoraConfig, TaskType\n",
    "from datasets import load_from_disk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. SETUP DATA & MODEL (Load Ulang Biar Bersih) ---\n",
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "dataset = load_from_disk(\"processed_data/emot_processed\") \n",
    "\n",
    "# Label Mapping untuk EMOT\n",
    "id2label = {0: \"sadness\", 1: \"anger\", 2: \"love\", 3: \"fear\", 4: \"happy\"}\n",
    "label2id = {\"sadness\": 0, \"anger\": 1, \"love\": 2, \"fear\": 3, \"happy\": 4}\n",
    "\n",
    "# Load Model Baru (PENTING: Jangan pakai variabel 'model' bekas SMSA)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, \n",
    "    num_labels=5,         # EMOT ada 5 label\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# --- 2. HITUNG ULANG STEPS (KHUSUS DATA EMOT) ---\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30 # Kita pakai 30 epoch sesuai strategi baru\n",
    "train_samples = len(dataset[\"train\"])\n",
    "\n",
    "# Rumus Steps\n",
    "total_steps = (train_samples // BATCH_SIZE) * NUM_EPOCHS\n",
    "t_init_steps = int(total_steps * 0.15) \n",
    "t_final_steps = int(total_steps * 0.20) \n",
    "\n",
    "print(f\"Data EMOT: {train_samples} sampel.\")\n",
    "print(f\"Jadwal AdaLoRA: Init={t_init_steps}, Final={t_final_steps}, Total={total_steps}\")\n",
    "\n",
    "# --- 3. KONFIGURASI ADALORA (KODEMU) ---\n",
    "peft_config = AdaLoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, \n",
    "    r=32,            \n",
    "    lora_alpha=64,   \n",
    "    target_modules=[\"query\", \"value\"], \n",
    "    lora_dropout=0.1, \n",
    "    bias=\"none\",\n",
    "    init_r=12, \n",
    "    target_r=8, \n",
    "    beta1=0.85, \n",
    "    beta2=0.85,\n",
    "    tinit=t_init_steps,     # Sekarang variabel ini sudah benar isinya\n",
    "    tfinal=t_final_steps,   # Sekarang variabel ini sudah benar isinya\n",
    "    deltaT=10,\n",
    "    total_step=total_steps  \n",
    ")\n",
    "\n",
    "# Bungkus model\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters() \n",
    "\n",
    "# --- 4. METRIC & TRAINER ---\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model_output/emot_adalora_tuned\",\n",
    "    learning_rate=5e-4, \n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\", \n",
    "    greater_is_better=True,           \n",
    "    save_total_limit=2,               \n",
    "    logging_steps=50,   \n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Custom Trainer (Fix Bug)\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        if \"num_items_in_batch\" in inputs:\n",
    "            del inputs[\"num_items_in_batch\"]\n",
    "        return super().compute_loss(model, inputs, return_outputs)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"], \n",
    "    eval_dataset=dataset[\"test\"], \n",
    "    tokenizer=AutoTokenizer.from_pretrained(checkpoint),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"üöÄ Mulai Training EMOT (Revisi Strategi)...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be54c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# BAGIAN 1: VISUALISASI KURVA BELAJAR (VERSI AMAN)\n",
    "# ==========================================\n",
    "print(\"Sedang membuat grafik Kurva Belajar...\")\n",
    "\n",
    "# 1. Ambil history training\n",
    "history = trainer.state.log_history\n",
    "df = pd.DataFrame(history)\n",
    "\n",
    "# 2. Pisahkan Data\n",
    "# Data Training (Loss)\n",
    "train_history = df[df['loss'].notna()]\n",
    "# Data Validation (Loss & Accuracy)\n",
    "val_history = df[df['eval_loss'].notna()]\n",
    "\n",
    "# 3. Plotting\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# --- Subplot 1: Grafik Loss ---\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "# Plot Training Loss (Sumbu X pakai Epoch)\n",
    "plt.plot(train_history['epoch'], train_history['loss'], \n",
    "         label='Training Loss', color='blue', alpha=0.4, linestyle='--')\n",
    "\n",
    "# Plot Validation Loss (Sumbu X pakai Epoch)\n",
    "plt.plot(val_history['epoch'], val_history['eval_loss'], \n",
    "         label='Validation Loss', color='red', linewidth=2, marker='o')\n",
    "\n",
    "plt.title('Kurva Loss: Training vs Validation', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# --- Subplot 2: Grafik Akurasi ---\n",
    "plt.subplot(1, 2, 2)\n",
    "# Plot Validation Accuracy\n",
    "plt.plot(val_history['epoch'], val_history['eval_accuracy'], \n",
    "         label='Validation Accuracy', color='green', linewidth=2, marker='s')\n",
    "\n",
    "plt.title('Kurva Akurasi (Validation)', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(bottom=0, top=1.0) # Skala 0 sampai 1\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff49f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# BAGIAN 1.5: LAKUKAN PREDIKSI DULU (INI YANG HILANG TADI)\n",
    "# ==========================================\n",
    "print(\"\\nSedang melakukan prediksi pada Data Test...\")\n",
    "\n",
    "# 1. Suruh Trainer memprediksi data test\n",
    "predictions_output = trainer.predict(dataset[\"test\"])\n",
    "\n",
    "# 2. Ambil hasil prediksi (y_pred) dan label asli (y_true)\n",
    "y_pred = np.argmax(predictions_output.predictions, axis=1)\n",
    "y_true = predictions_output.label_ids\n",
    "\n",
    "# 3. Definisi Label (Harus urut 0-4 sesuai training)\n",
    "label_names = [\"Sadness (0)\", \"Anger (1)\", \"Love (2)\", \"Fear (3)\", \"Happy (4)\"]\n",
    "\n",
    "# ==========================================\n",
    "# BAGIAN 2: VISUALISASI CONFUSION MATRIX\n",
    "# ==========================================\n",
    "print(\"Sedang membuat Confusion Matrix...\")\n",
    "\n",
    "# 4. Hitung Matriks\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# 5. Plot Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_names, yticklabels=label_names,\n",
    "            cbar_kws={'label': 'Jumlah Sampel'}, annot_kws={\"size\": 12})\n",
    "\n",
    "plt.title('Confusion Matrix: Detail Kesalahan Prediksi EMOT', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Prediksi Model', fontsize=12)\n",
    "plt.ylabel('Label Sebenarnya (Kunci Jawaban)', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# BAGIAN 3: LAPORAN ANGKA DETAIL (CLASSIFICATION REPORT)\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DETAIL PERFORMA PER KATEGORI EMOSI\")\n",
    "print(\"=\"*50)\n",
    "# Menampilkan Precision, Recall, dan F1-Score untuk setiap emosi\n",
    "report = classification_report(y_true, y_pred, target_names=label_names, digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d4cf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Nama folder penyimpanan\n",
    "save_path = \"saved_models/emot_adalora_best\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# 1. Simpan Model Adapter (Best Model)\n",
    "trainer.save_model(save_path)\n",
    "\n",
    "# 2. Simpan Tokenizer (Wajib sepaket)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"‚úÖ Model EMOT Terbaik sukses disimpan di: {save_path}\")\n",
    "\n",
    "# Cek model dari epoch berapa yang diambil\n",
    "if trainer.state.best_model_checkpoint:\n",
    "    print(f\"‚ÑπÔ∏è Model ini adalah juara dari: {trainer.state.best_model_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd9c44c",
   "metadata": {},
   "source": [
    "Nerp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d5a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cek Metadata Label\n",
    "try:\n",
    "    # Biasanya tersimpan di fitur 'ner_tags' atau 'labels'\n",
    "    if \"ner_tags\" in dataset[\"train\"].features:\n",
    "        labels_names = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "    else:\n",
    "        labels_names = dataset[\"train\"].features[\"labels\"].feature.names\n",
    "        \n",
    "    print(\"‚úÖ BERHASIL DITEMUKAN!\")\n",
    "    print(f\"Total Label: {len(labels_names)}\")\n",
    "    print(f\"Daftar Label: {labels_names}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Gagal membaca metadata otomatis.\")\n",
    "    print(\"Label tersimpan sebagai angka mentah. Kita harus pakai nama sementara.\")\n",
    "    # Fallback: Buat label dummy biar training tetap jalan\n",
    "    labels_names = [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\", \"LABEL_9\", \"LABEL_10\"]\n",
    "    print(f\"Saran Label List Sementara: {labels_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1459345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification, \n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import get_peft_model, AdaLoraConfig, TaskType\n",
    "from datasets import load_from_disk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "\n",
    "# ==========================================\n",
    "# 1. PERSIAPAN DATA (PAKSA SPLIT 80:20)\n",
    "# ==========================================\n",
    "print(\"üöÄ Memulai Pipeline Training NERP...\")\n",
    "\n",
    "# Load Data Mentah\n",
    "try:\n",
    "    dataset_raw = load_from_disk(\"processed_data/nerp_processed\")\n",
    "    print(\"‚úÖ Dataset raw berhasil diload.\")\n",
    "except:\n",
    "    raise ValueError(\"‚ùå Folder 'processed_data/nerp_processed' tidak ditemukan!\")\n",
    "\n",
    "# --- LOGIKA BARU: FORCE RESPLIT 0.2 ---\n",
    "print(\"üîÑ Sedang membagi ulang dataset menjadi 80% Train : 20% Test...\")\n",
    "\n",
    "# Cek apakah dataset ini berbentuk Dictionary (sudah ada train/test-nya)\n",
    "if isinstance(dataset_raw, dict) or hasattr(dataset_raw, \"keys\"):\n",
    "    keys = list(dataset_raw.keys())\n",
    "    if \"train\" in keys and \"test\" in keys:\n",
    "        # Kasus: Sudah terlanjur dibagi (misal 90/10), kita gabung dulu\n",
    "        full_data = concatenate_datasets([dataset_raw[\"train\"], dataset_raw[\"test\"]])\n",
    "        dataset = full_data.train_test_split(test_size=0.2) # <--- DISINI KITA UBAH JADI 0.2\n",
    "    elif \"train\" in keys:\n",
    "        # Kasus: Cuma ada train, langsung split\n",
    "        dataset = dataset_raw[\"train\"].train_test_split(test_size=0.2)\n",
    "    else:\n",
    "        # Kasus: Dataset tunggal (belum diapa-apain)\n",
    "        dataset = dataset_raw.train_test_split(test_size=0.2)\n",
    "else:\n",
    "    # Jaga-jaga kalau formatnya lain\n",
    "    dataset = dataset_raw.train_test_split(test_size=0.2)\n",
    "\n",
    "# Validasi Kolom (Anti-Gado-Gado)\n",
    "sample_cols = dataset[\"train\"].column_names\n",
    "if \"labels\" not in sample_cols and \"ner_tags\" not in sample_cols:\n",
    "    raise ValueError(f\"‚ùå INI BUKAN DATA NER! Kolom: {sample_cols}\")\n",
    "\n",
    "print(f\"‚úÖ DATA READY! Train: {len(dataset['train'])}, Test: {len(dataset['test'])}\")\n",
    "print(f\"   (Rasio Test: {len(dataset['test']) / (len(dataset['train']) + len(dataset['test'])):.2%})\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. CONFIG MODEL & LABEL\n",
    "# ==========================================\n",
    "# Definisi 11 Label (Sesuaikan urutan jika beda)\n",
    "label_list = [\n",
    "    \"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \n",
    "    \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\", \n",
    "    \"LABEL_9\", \"LABEL_10\"\n",
    "]\n",
    "id2label = {i: l for i, l in enumerate(label_list)}\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "\n",
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# Load Base Model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    checkpoint, num_labels=len(label_list), id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "# Config AdaLoRA (STRICT NER MODE)\n",
    "peft_config = AdaLoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,  # <--- WAJIB TOKEN_CLS UNTUK NER\n",
    "    inference_mode=False, \n",
    "    r=16, lora_alpha=32, lora_dropout=0.1, bias=\"none\",\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    init_r=12, target_r=8, beta1=0.85, beta2=0.85,\n",
    "    tinit=200, tfinal=1000, deltaT=10, total_step=10000 \n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# ==========================================\n",
    "# 3. METRIK & TRAINING\n",
    "# ==========================================\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Filter -100 (Padding)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# Custom Trainer (Anti-Bug)\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        if \"num_items_in_batch\" in inputs: del inputs[\"num_items_in_batch\"]\n",
    "        return super().compute_loss(model, inputs, return_outputs)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model_output/nerp_adalora_final\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10, # Coba 10 epoch biar kelihatan grafiknya\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\", # Versi baru Transformers\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics # Metrik dimasukkan disini\n",
    ")\n",
    "\n",
    "print(\"\\n‚è≥ Sedang Melatih Model NER...\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6ff09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SIMPAN MODEL\n",
    "save_path = \"saved_models/nerp_adalora_best\"\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"\\nüíæ Model Berhasil Disimpan di: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464862a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A. Kurva Loss & Accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Grafik Loss (Kiri)\n",
    "plt.subplot(1, 2, 1)\n",
    "# Plot Training Loss (Garis biru putus-putus)\n",
    "loss_data = history.dropna(subset=['loss'])\n",
    "plt.plot(loss_data['epoch'], loss_data['loss'], label='Train Loss', alpha=0.6, linestyle='--')\n",
    "\n",
    "# Plot Validation Loss (Garis merah bulat)\n",
    "if 'eval_loss' in history.columns: \n",
    "    val_loss_data = history.dropna(subset=['eval_loss'])\n",
    "    plt.plot(val_loss_data['epoch'], val_loss_data['eval_loss'], label='Val Loss', marker='o', color='red')\n",
    "\n",
    "plt.title(\"Kurva Loss (Semakin Rendah Semakin Baik)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Grafik Accuracy (Kanan) - PENGGANTI F1\n",
    "plt.subplot(1, 2, 2)\n",
    "if 'eval_accuracy' in history.columns:\n",
    "    val_acc_data = history.dropna(subset=['eval_accuracy'])\n",
    "    plt.plot(val_acc_data['epoch'], val_acc_data['eval_accuracy'], label='Val Accuracy', marker='s', color='orange')\n",
    "    plt.title(\"Kurva Accuracy (Semakin Tinggi Semakin Baik)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "else:\n",
    "    # Jaga-jaga kalau key-nya beda, kadang bisa 'eval_overall_accuracy' tergantung versi library\n",
    "    print(\"‚ö†Ô∏è Kolom 'eval_accuracy' tidak ditemukan di history. Cek nama kolom history.columns\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5096fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# B. Confusion Matrix\n",
    "print(\"\\nüìä Membuat Confusion Matrix...\")\n",
    "predictions, labels, _ = trainer.predict(dataset[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "true_labels = []\n",
    "true_preds = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels[i])):\n",
    "        if labels[i][j] != -100:\n",
    "            true_labels.append(label_list[labels[i][j]])\n",
    "            true_preds.append(label_list[predictions[i][j]])\n",
    "\n",
    "unique_labels = sorted(list(set(true_labels + true_preds)))\n",
    "display_labels = [l for l in unique_labels if \"LABEL\" not in l] # Buang label sampah\n",
    "\n",
    "cm = confusion_matrix(true_labels, true_preds, labels=display_labels)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=display_labels, yticklabels=display_labels)\n",
    "plt.title('Confusion Matrix NER')\n",
    "plt.ylabel('Asli')\n",
    "plt.xlabel('Prediksi')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüèÜ DETAIL RAPOR:\")\n",
    "print(classification_report(true_labels, true_preds, labels=display_labels, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b585a1",
   "metadata": {},
   "source": [
    "QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed9d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForQuestionAnswering, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DefaultDataCollator\n",
    ")\n",
    "from peft import get_peft_model, AdaLoraConfig, TaskType\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. SETUP DATA & MODEL ---\n",
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "dataset_raw = load_from_disk(\"processed_data/squad_processed\") \n",
    "\n",
    "# --- PERBAIKAN DI SINI ---\n",
    "# Karena dataset_raw tidak punya key 'train', kita buat split sendiri\n",
    "# 90% buat Training, 10% buat Validasi/Ujian\n",
    "print(\"Sedang membagi dataset menjadi Train & Validation...\")\n",
    "dataset = dataset_raw.train_test_split(test_size=0.1)\n",
    "\n",
    "print(f\"‚úÖ Data Siap! Train: {len(dataset['train'])}, Test: {len(dataset['test'])}\")\n",
    "\n",
    "# Load Model QA\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)\n",
    "\n",
    "# --- 2. CONFIG ADALORA ---\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20 \n",
    "train_samples = len(dataset[\"train\"]) # Sekarang sudah aman pakai key 'train'\n",
    "\n",
    "# Rumus Steps\n",
    "total_steps = (train_samples // BATCH_SIZE) * NUM_EPOCHS\n",
    "\n",
    "peft_config = AdaLoraConfig(\n",
    "    task_type=TaskType.QUESTION_ANS, # <--- PENTING: QA\n",
    "    inference_mode=False, \n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.01, \n",
    "    bias=\"none\",\n",
    "    init_r=12, \n",
    "    target_r=8, \n",
    "    beta1=0.85, \n",
    "    beta2=0.85, \n",
    "    tinit=int(total_steps * 0.1),  \n",
    "    tfinal=int(total_steps * 0.2), \n",
    "    deltaT=10, \n",
    "    total_step=total_steps\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# --- 3. DATA COLLATOR ---\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "# --- 4. CUSTOM TRAINER (ANTI BUG) ---\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        if \"num_items_in_batch\" in inputs:\n",
    "            del inputs[\"num_items_in_batch\"]\n",
    "        return super().compute_loss(model, inputs, return_outputs)\n",
    "\n",
    "# --- 5. TRAINING ARGUMENTS ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model_output/squad_adalora\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer( \n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"], # Ambil dari hasil split tadi\n",
    "    eval_dataset=dataset[\"test\"],   # Ambil dari hasil split tadi\n",
    "    tokenizer=AutoTokenizer.from_pretrained(checkpoint),\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Mulai Training QA SQuAD...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3d5fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# ==========================================\n",
    "# 1. VISUALISASI KURVA TRAINING (VERSI AMAN)\n",
    "# ==========================================\n",
    "print(\"üìä Sedang membuat Grafik Kurva Belajar NERP...\")\n",
    "\n",
    "# 1. Ambil history\n",
    "history = trainer.state.log_history\n",
    "df = pd.DataFrame(history)\n",
    "\n",
    "# 2. Pisahkan Data\n",
    "train_history = df[df['loss'].notna()]\n",
    "val_history = df[df['eval_loss'].notna()]\n",
    "\n",
    "# --- DEBUGGING: CEK KOLOM APA YANG ADA ---\n",
    "print(\"Kolom yang tersedia di log validasi:\", val_history.columns.tolist())\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# --- Grafik 1: LOSS ---\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_history['epoch'], train_history['loss'], \n",
    "         label='Training Loss', color='blue', alpha=0.4, linestyle='--')\n",
    "plt.plot(val_history['epoch'], val_history['eval_loss'], \n",
    "         label='Validation Loss', color='red', linewidth=2, marker='o')\n",
    "\n",
    "plt.title('Kurva Loss (Makin Rendah Makin Bagus)', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b7bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, DefaultDataCollator\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP DATA QA (YANG SUDAH BENAR)\n",
    "# ==========================================\n",
    "print(\"üöÄ Mempersiapkan Data QA...\")\n",
    "\n",
    "# Load Data (Ini adalah Dataset QA Single)\n",
    "dataset_raw = load_from_disk(\"processed_data/squad_processed\") # Pastikan path ini benar\n",
    "\n",
    "# KITA PECAH JADI DUA (Train & Test)\n",
    "# Biar bisa dipanggil dataset['train'] dan dataset['test']\n",
    "dataset = dataset_raw.train_test_split(test_size=0.1) \n",
    "print(f\"‚úÖ Data Siap! Train: {len(dataset['train'])}, Test: {len(dataset['test'])}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. SETUP MODEL & TOKENIZER\n",
    "# ==========================================\n",
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Pakai Base Model QA (Belum fine-tuned, jadi jawaban mungkin masih ngaco, tapi formatnya benar)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ==========================================\n",
    "# 3. VISUALISASI JAWABAN\n",
    "# ==========================================\n",
    "print(\"\\nüîç Mulai Visualisasi QA...\")\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "# Ambil 10 sampel dari data test\n",
    "eval_loader = DataLoader(dataset[\"test\"].select(range(10)), batch_size=1, collate_fn=data_collator)\n",
    "\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(eval_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward Pass (Model QA)\n",
    "        outputs = model(**batch)\n",
    "        start_logits = outputs.start_logits\n",
    "        end_logits = outputs.end_logits\n",
    "        \n",
    "        # 1. PREDISKI MODEL\n",
    "        answer_start = torch.argmax(start_logits)\n",
    "        answer_end = torch.argmax(end_logits) + 1\n",
    "        \n",
    "        input_ids = batch[\"input_ids\"][0].cpu().tolist()\n",
    "        pred_tokens = input_ids[answer_start : answer_end]\n",
    "        pred_text = tokenizer.decode(pred_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        # 2. KUNCI JAWABAN (DARI DATASET)\n",
    "        # Ambil posisi start/end asli dari data\n",
    "        if \"start_positions\" in batch:\n",
    "            true_start = batch[\"start_positions\"][0].item()\n",
    "            true_end = batch[\"end_positions\"][0].item() + 1\n",
    "            true_tokens = input_ids[true_start : true_end]\n",
    "            true_text = tokenizer.decode(true_tokens, skip_special_tokens=True)\n",
    "        else:\n",
    "            true_text = \"N/A\"\n",
    "\n",
    "        results.append({\n",
    "            \"Jawaban Model\": pred_text,\n",
    "            \"Kunci Jawaban\": true_text,\n",
    "            \"Cek\": \"‚úÖ\" if pred_text == true_text and pred_text != \"\" else \"‚ùå\"\n",
    "        })\n",
    "\n",
    "# Tampilkan Tabel\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\n=== HASIL VISUALISASI QA (SQuAD) ===\")\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec38db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "save_path = \"saved_models/squad_adalora_best\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "trainer.save_model(save_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"‚úÖ Model QA tersimpan di: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f71890",
   "metadata": {},
   "source": [
    "NLG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ef46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM, \n",
    "    AutoTokenizer, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import get_peft_model, AdaLoraConfig, TaskType\n",
    "from datasets import load_from_disk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. SETUP DATA & MODEL ---\n",
    "model_checkpoint = \"google/mt5-small\"\n",
    "dataset_raw = load_from_disk(\"processed_data/wikilingua_mt5_processed\") \n",
    "\n",
    "# --- CEK STRUKTUR & PERBAIKAN ---\n",
    "print(f\"Struktur Awal: {dataset_raw}\")\n",
    "\n",
    "# Jika dataset bukan Dictionary (tidak punya key 'train'), kita split manual\n",
    "if \"train\" not in dataset_raw:\n",
    "    print(\"‚ö†Ô∏è Dataset belum di-split. Melakukan split otomatis 90/10...\")\n",
    "    dataset = dataset_raw.train_test_split(test_size=0.1)\n",
    "else:\n",
    "    dataset = dataset_raw\n",
    "\n",
    "print(f\"‚úÖ Data Siap! Train: {len(dataset['train'])}, Test: {len(dataset['test'])}\")\n",
    "\n",
    "# Load Tokenizer & Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# --- 2. CONFIG ADALORA (Khusus T5) ---\n",
    "BATCH_SIZE = 8 \n",
    "NUM_EPOCHS = 10 \n",
    "# Sekarang aman akses [\"train\"] karena sudah di-split di atas\n",
    "train_samples = len(dataset[\"train\"]) \n",
    "total_steps = (train_samples // BATCH_SIZE) * NUM_EPOCHS\n",
    "\n",
    "peft_config = AdaLoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM, # <--- Task Type SEQ2SEQ\n",
    "    inference_mode=False, \n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"q\", \"v\"], # Target Module T5\n",
    "    lora_dropout=0.01, \n",
    "    bias=\"none\",\n",
    "    init_r=12, \n",
    "    target_r=8, \n",
    "    beta1=0.85, \n",
    "    beta2=0.85, \n",
    "    tinit=int(total_steps * 0.1),  \n",
    "    tfinal=int(total_steps * 0.2), \n",
    "    deltaT=10, \n",
    "    total_step=total_steps\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# --- 3. DATA COLLATOR ---\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# --- 4. METRIC (ROUGE) ---\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Ganti -100 dengan pad token\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "# --- 5. TRAINING ARGUMENTS ---\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"model_output/wikilingua_adalora\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    predict_with_generate=True, # Wajib untuk summarization\n",
    "    fp16=False,                 \n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# --- 6. TRAINER ---\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Mulai Training Summarization...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e42409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Simpan\n",
    "save_path = \"saved_models/wikilingua_adalora_best\"\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(\"‚úÖ Model Summarization Tersimpan!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
