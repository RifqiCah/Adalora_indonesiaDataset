{
  "best_global_step": 3442,
  "best_metric": 3.083611249923706,
  "best_model_checkpoint": "model_output/squad_lora_baseline\\checkpoint-3442",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 3442,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002905287623474724,
      "grad_norm": 2.2900285720825195,
      "learning_rate": 0.0009973852411388727,
      "loss": 5.7623,
      "step": 10
    },
    {
      "epoch": 0.005810575246949448,
      "grad_norm": 4.7035908699035645,
      "learning_rate": 0.000994479953515398,
      "loss": 5.0511,
      "step": 20
    },
    {
      "epoch": 0.008715862870424172,
      "grad_norm": 2.4123547077178955,
      "learning_rate": 0.0009915746658919233,
      "loss": 4.3965,
      "step": 30
    },
    {
      "epoch": 0.011621150493898896,
      "grad_norm": 2.164633274078369,
      "learning_rate": 0.0009886693782684486,
      "loss": 3.9331,
      "step": 40
    },
    {
      "epoch": 0.01452643811737362,
      "grad_norm": 1.3925089836120605,
      "learning_rate": 0.0009857640906449738,
      "loss": 3.8787,
      "step": 50
    },
    {
      "epoch": 0.017431725740848343,
      "grad_norm": 1.870766520500183,
      "learning_rate": 0.0009828588030214992,
      "loss": 3.6273,
      "step": 60
    },
    {
      "epoch": 0.02033701336432307,
      "grad_norm": 1.6806590557098389,
      "learning_rate": 0.0009799535153980244,
      "loss": 3.9528,
      "step": 70
    },
    {
      "epoch": 0.023242300987797792,
      "grad_norm": 1.82172429561615,
      "learning_rate": 0.0009770482277745498,
      "loss": 3.8221,
      "step": 80
    },
    {
      "epoch": 0.026147588611272515,
      "grad_norm": 1.4435300827026367,
      "learning_rate": 0.0009741429401510749,
      "loss": 3.8275,
      "step": 90
    },
    {
      "epoch": 0.02905287623474724,
      "grad_norm": 2.0265581607818604,
      "learning_rate": 0.0009712376525276002,
      "loss": 3.7862,
      "step": 100
    },
    {
      "epoch": 0.031958163858221963,
      "grad_norm": 2.6457600593566895,
      "learning_rate": 0.0009683323649041256,
      "loss": 3.5711,
      "step": 110
    },
    {
      "epoch": 0.034863451481696686,
      "grad_norm": 1.3197427988052368,
      "learning_rate": 0.0009654270772806508,
      "loss": 3.8756,
      "step": 120
    },
    {
      "epoch": 0.03776873910517141,
      "grad_norm": 1.7021979093551636,
      "learning_rate": 0.0009625217896571761,
      "loss": 3.6834,
      "step": 130
    },
    {
      "epoch": 0.04067402672864614,
      "grad_norm": 1.5143944025039673,
      "learning_rate": 0.0009596165020337013,
      "loss": 3.269,
      "step": 140
    },
    {
      "epoch": 0.04357931435212086,
      "grad_norm": 1.3824872970581055,
      "learning_rate": 0.0009567112144102267,
      "loss": 3.5919,
      "step": 150
    },
    {
      "epoch": 0.046484601975595584,
      "grad_norm": 1.5125176906585693,
      "learning_rate": 0.0009538059267867519,
      "loss": 3.3592,
      "step": 160
    },
    {
      "epoch": 0.04938988959907031,
      "grad_norm": 1.6267868280410767,
      "learning_rate": 0.0009509006391632772,
      "loss": 3.4454,
      "step": 170
    },
    {
      "epoch": 0.05229517722254503,
      "grad_norm": 1.487682580947876,
      "learning_rate": 0.0009479953515398025,
      "loss": 3.479,
      "step": 180
    },
    {
      "epoch": 0.05520046484601976,
      "grad_norm": 1.6101469993591309,
      "learning_rate": 0.0009450900639163278,
      "loss": 3.5923,
      "step": 190
    },
    {
      "epoch": 0.05810575246949448,
      "grad_norm": 1.4064029455184937,
      "learning_rate": 0.000942184776292853,
      "loss": 3.4221,
      "step": 200
    },
    {
      "epoch": 0.061011040092969204,
      "grad_norm": 1.4371259212493896,
      "learning_rate": 0.0009392794886693783,
      "loss": 3.4003,
      "step": 210
    },
    {
      "epoch": 0.06391632771644393,
      "grad_norm": 1.9716442823410034,
      "learning_rate": 0.0009363742010459036,
      "loss": 3.4386,
      "step": 220
    },
    {
      "epoch": 0.06682161533991865,
      "grad_norm": 1.821012020111084,
      "learning_rate": 0.0009334689134224287,
      "loss": 3.377,
      "step": 230
    },
    {
      "epoch": 0.06972690296339337,
      "grad_norm": 1.676971435546875,
      "learning_rate": 0.0009305636257989541,
      "loss": 3.2356,
      "step": 240
    },
    {
      "epoch": 0.0726321905868681,
      "grad_norm": 1.6900616884231567,
      "learning_rate": 0.0009276583381754794,
      "loss": 3.3411,
      "step": 250
    },
    {
      "epoch": 0.07553747821034282,
      "grad_norm": 1.579468846321106,
      "learning_rate": 0.0009247530505520047,
      "loss": 3.4715,
      "step": 260
    },
    {
      "epoch": 0.07844276583381755,
      "grad_norm": 1.6163780689239502,
      "learning_rate": 0.0009218477629285299,
      "loss": 3.6581,
      "step": 270
    },
    {
      "epoch": 0.08134805345729228,
      "grad_norm": 1.50530207157135,
      "learning_rate": 0.0009189424753050553,
      "loss": 3.3552,
      "step": 280
    },
    {
      "epoch": 0.084253341080767,
      "grad_norm": 1.5594544410705566,
      "learning_rate": 0.0009160371876815805,
      "loss": 3.5475,
      "step": 290
    },
    {
      "epoch": 0.08715862870424172,
      "grad_norm": 1.598235011100769,
      "learning_rate": 0.0009131319000581057,
      "loss": 3.2269,
      "step": 300
    },
    {
      "epoch": 0.09006391632771645,
      "grad_norm": 2.4725863933563232,
      "learning_rate": 0.000910226612434631,
      "loss": 3.4113,
      "step": 310
    },
    {
      "epoch": 0.09296920395119117,
      "grad_norm": 1.6364535093307495,
      "learning_rate": 0.0009073213248111564,
      "loss": 3.5997,
      "step": 320
    },
    {
      "epoch": 0.09587449157466589,
      "grad_norm": 1.8439812660217285,
      "learning_rate": 0.0009044160371876817,
      "loss": 3.3001,
      "step": 330
    },
    {
      "epoch": 0.09877977919814061,
      "grad_norm": 1.902074933052063,
      "learning_rate": 0.0009015107495642068,
      "loss": 3.5325,
      "step": 340
    },
    {
      "epoch": 0.10168506682161534,
      "grad_norm": 2.8686459064483643,
      "learning_rate": 0.0008986054619407321,
      "loss": 3.2124,
      "step": 350
    },
    {
      "epoch": 0.10459035444509006,
      "grad_norm": 2.220883369445801,
      "learning_rate": 0.0008957001743172575,
      "loss": 3.392,
      "step": 360
    },
    {
      "epoch": 0.10749564206856478,
      "grad_norm": 1.7795203924179077,
      "learning_rate": 0.0008927948866937828,
      "loss": 3.4182,
      "step": 370
    },
    {
      "epoch": 0.11040092969203952,
      "grad_norm": 1.8273403644561768,
      "learning_rate": 0.0008898895990703079,
      "loss": 3.321,
      "step": 380
    },
    {
      "epoch": 0.11330621731551424,
      "grad_norm": 1.8778537511825562,
      "learning_rate": 0.0008869843114468332,
      "loss": 3.291,
      "step": 390
    },
    {
      "epoch": 0.11621150493898896,
      "grad_norm": 1.8220536708831787,
      "learning_rate": 0.0008840790238233586,
      "loss": 3.2884,
      "step": 400
    },
    {
      "epoch": 0.11911679256246369,
      "grad_norm": 1.5115597248077393,
      "learning_rate": 0.0008811737361998838,
      "loss": 3.1913,
      "step": 410
    },
    {
      "epoch": 0.12202208018593841,
      "grad_norm": 2.170912265777588,
      "learning_rate": 0.0008782684485764091,
      "loss": 3.5429,
      "step": 420
    },
    {
      "epoch": 0.12492736780941313,
      "grad_norm": 2.3419294357299805,
      "learning_rate": 0.0008753631609529343,
      "loss": 3.3413,
      "step": 430
    },
    {
      "epoch": 0.12783265543288785,
      "grad_norm": 1.5325961112976074,
      "learning_rate": 0.0008724578733294596,
      "loss": 3.3537,
      "step": 440
    },
    {
      "epoch": 0.13073794305636258,
      "grad_norm": 1.5520018339157104,
      "learning_rate": 0.0008695525857059849,
      "loss": 3.3755,
      "step": 450
    },
    {
      "epoch": 0.1336432306798373,
      "grad_norm": 2.2777912616729736,
      "learning_rate": 0.0008666472980825102,
      "loss": 3.3373,
      "step": 460
    },
    {
      "epoch": 0.13654851830331202,
      "grad_norm": 1.74458909034729,
      "learning_rate": 0.0008637420104590355,
      "loss": 3.276,
      "step": 470
    },
    {
      "epoch": 0.13945380592678674,
      "grad_norm": 1.5568767786026,
      "learning_rate": 0.0008608367228355607,
      "loss": 3.2382,
      "step": 480
    },
    {
      "epoch": 0.14235909355026147,
      "grad_norm": 1.4906760454177856,
      "learning_rate": 0.000857931435212086,
      "loss": 3.4619,
      "step": 490
    },
    {
      "epoch": 0.1452643811737362,
      "grad_norm": 1.4658621549606323,
      "learning_rate": 0.0008550261475886113,
      "loss": 3.3808,
      "step": 500
    },
    {
      "epoch": 0.1481696687972109,
      "grad_norm": 1.4012562036514282,
      "learning_rate": 0.0008521208599651366,
      "loss": 3.3217,
      "step": 510
    },
    {
      "epoch": 0.15107495642068564,
      "grad_norm": 2.1567111015319824,
      "learning_rate": 0.0008492155723416618,
      "loss": 3.3102,
      "step": 520
    },
    {
      "epoch": 0.15398024404416036,
      "grad_norm": 2.405667543411255,
      "learning_rate": 0.0008463102847181871,
      "loss": 3.3304,
      "step": 530
    },
    {
      "epoch": 0.1568855316676351,
      "grad_norm": 1.5223475694656372,
      "learning_rate": 0.0008434049970947124,
      "loss": 3.2606,
      "step": 540
    },
    {
      "epoch": 0.15979081929110983,
      "grad_norm": 2.315023899078369,
      "learning_rate": 0.0008404997094712377,
      "loss": 3.4948,
      "step": 550
    },
    {
      "epoch": 0.16269610691458455,
      "grad_norm": 1.3691843748092651,
      "learning_rate": 0.0008375944218477629,
      "loss": 3.2258,
      "step": 560
    },
    {
      "epoch": 0.16560139453805928,
      "grad_norm": 2.4690918922424316,
      "learning_rate": 0.0008346891342242883,
      "loss": 3.2007,
      "step": 570
    },
    {
      "epoch": 0.168506682161534,
      "grad_norm": 1.382073163986206,
      "learning_rate": 0.0008317838466008135,
      "loss": 3.3138,
      "step": 580
    },
    {
      "epoch": 0.17141196978500872,
      "grad_norm": 1.849840760231018,
      "learning_rate": 0.0008288785589773388,
      "loss": 3.2435,
      "step": 590
    },
    {
      "epoch": 0.17431725740848344,
      "grad_norm": 1.5866891145706177,
      "learning_rate": 0.000825973271353864,
      "loss": 3.3808,
      "step": 600
    },
    {
      "epoch": 0.17722254503195817,
      "grad_norm": 2.3325705528259277,
      "learning_rate": 0.0008230679837303894,
      "loss": 3.3745,
      "step": 610
    },
    {
      "epoch": 0.1801278326554329,
      "grad_norm": 1.4847465753555298,
      "learning_rate": 0.0008201626961069147,
      "loss": 3.2028,
      "step": 620
    },
    {
      "epoch": 0.1830331202789076,
      "grad_norm": 1.5712686777114868,
      "learning_rate": 0.0008172574084834398,
      "loss": 3.333,
      "step": 630
    },
    {
      "epoch": 0.18593840790238234,
      "grad_norm": 1.4446742534637451,
      "learning_rate": 0.0008143521208599651,
      "loss": 3.4188,
      "step": 640
    },
    {
      "epoch": 0.18884369552585706,
      "grad_norm": 2.428452968597412,
      "learning_rate": 0.0008114468332364904,
      "loss": 3.362,
      "step": 650
    },
    {
      "epoch": 0.19174898314933178,
      "grad_norm": 1.9187860488891602,
      "learning_rate": 0.0008085415456130158,
      "loss": 3.4743,
      "step": 660
    },
    {
      "epoch": 0.1946542707728065,
      "grad_norm": 1.5960088968276978,
      "learning_rate": 0.000805636257989541,
      "loss": 3.4406,
      "step": 670
    },
    {
      "epoch": 0.19755955839628123,
      "grad_norm": 1.7939327955245972,
      "learning_rate": 0.0008027309703660662,
      "loss": 3.0766,
      "step": 680
    },
    {
      "epoch": 0.20046484601975595,
      "grad_norm": 1.353044867515564,
      "learning_rate": 0.0007998256827425915,
      "loss": 3.3694,
      "step": 690
    },
    {
      "epoch": 0.20337013364323067,
      "grad_norm": 2.150677442550659,
      "learning_rate": 0.0007969203951191168,
      "loss": 3.2831,
      "step": 700
    },
    {
      "epoch": 0.2062754212667054,
      "grad_norm": 1.9391729831695557,
      "learning_rate": 0.0007940151074956421,
      "loss": 3.2648,
      "step": 710
    },
    {
      "epoch": 0.20918070889018012,
      "grad_norm": 1.5942765474319458,
      "learning_rate": 0.0007911098198721674,
      "loss": 3.3887,
      "step": 720
    },
    {
      "epoch": 0.21208599651365484,
      "grad_norm": 1.660500407218933,
      "learning_rate": 0.0007882045322486926,
      "loss": 3.5124,
      "step": 730
    },
    {
      "epoch": 0.21499128413712956,
      "grad_norm": 1.3972883224487305,
      "learning_rate": 0.0007852992446252179,
      "loss": 3.3776,
      "step": 740
    },
    {
      "epoch": 0.2178965717606043,
      "grad_norm": 1.9437823295593262,
      "learning_rate": 0.0007823939570017432,
      "loss": 3.3611,
      "step": 750
    },
    {
      "epoch": 0.22080185938407904,
      "grad_norm": 1.7179937362670898,
      "learning_rate": 0.0007794886693782685,
      "loss": 3.361,
      "step": 760
    },
    {
      "epoch": 0.22370714700755376,
      "grad_norm": 1.9483001232147217,
      "learning_rate": 0.0007765833817547938,
      "loss": 3.4194,
      "step": 770
    },
    {
      "epoch": 0.22661243463102848,
      "grad_norm": 1.7964493036270142,
      "learning_rate": 0.000773678094131319,
      "loss": 3.3758,
      "step": 780
    },
    {
      "epoch": 0.2295177222545032,
      "grad_norm": 1.5681288242340088,
      "learning_rate": 0.0007707728065078443,
      "loss": 3.1687,
      "step": 790
    },
    {
      "epoch": 0.23242300987797793,
      "grad_norm": 2.9333655834198,
      "learning_rate": 0.0007678675188843696,
      "loss": 3.5414,
      "step": 800
    },
    {
      "epoch": 0.23532829750145265,
      "grad_norm": 1.637184977531433,
      "learning_rate": 0.0007649622312608948,
      "loss": 3.3168,
      "step": 810
    },
    {
      "epoch": 0.23823358512492737,
      "grad_norm": 1.387618899345398,
      "learning_rate": 0.0007620569436374202,
      "loss": 3.1164,
      "step": 820
    },
    {
      "epoch": 0.2411388727484021,
      "grad_norm": 1.438273549079895,
      "learning_rate": 0.0007591516560139454,
      "loss": 3.2684,
      "step": 830
    },
    {
      "epoch": 0.24404416037187682,
      "grad_norm": 1.8993808031082153,
      "learning_rate": 0.0007562463683904707,
      "loss": 3.4241,
      "step": 840
    },
    {
      "epoch": 0.24694944799535154,
      "grad_norm": 1.459004521369934,
      "learning_rate": 0.0007533410807669959,
      "loss": 3.3228,
      "step": 850
    },
    {
      "epoch": 0.24985473561882626,
      "grad_norm": 2.9291603565216064,
      "learning_rate": 0.0007504357931435212,
      "loss": 3.2371,
      "step": 860
    },
    {
      "epoch": 0.252760023242301,
      "grad_norm": 2.5066239833831787,
      "learning_rate": 0.0007475305055200466,
      "loss": 3.2153,
      "step": 870
    },
    {
      "epoch": 0.2556653108657757,
      "grad_norm": 1.368129849433899,
      "learning_rate": 0.0007446252178965718,
      "loss": 3.055,
      "step": 880
    },
    {
      "epoch": 0.25857059848925046,
      "grad_norm": 1.8616340160369873,
      "learning_rate": 0.000741719930273097,
      "loss": 3.1837,
      "step": 890
    },
    {
      "epoch": 0.26147588611272515,
      "grad_norm": 2.615665912628174,
      "learning_rate": 0.0007388146426496223,
      "loss": 3.2123,
      "step": 900
    },
    {
      "epoch": 0.2643811737361999,
      "grad_norm": 2.0531632900238037,
      "learning_rate": 0.0007359093550261477,
      "loss": 3.0978,
      "step": 910
    },
    {
      "epoch": 0.2672864613596746,
      "grad_norm": 2.5102345943450928,
      "learning_rate": 0.0007330040674026728,
      "loss": 3.1809,
      "step": 920
    },
    {
      "epoch": 0.27019174898314935,
      "grad_norm": 1.3491798639297485,
      "learning_rate": 0.0007300987797791981,
      "loss": 3.3271,
      "step": 930
    },
    {
      "epoch": 0.27309703660662404,
      "grad_norm": 1.4604876041412354,
      "learning_rate": 0.0007271934921557234,
      "loss": 3.2561,
      "step": 940
    },
    {
      "epoch": 0.2760023242300988,
      "grad_norm": 1.3914302587509155,
      "learning_rate": 0.0007242882045322488,
      "loss": 3.4059,
      "step": 950
    },
    {
      "epoch": 0.2789076118535735,
      "grad_norm": 1.4208427667617798,
      "learning_rate": 0.000721382916908774,
      "loss": 3.3774,
      "step": 960
    },
    {
      "epoch": 0.28181289947704824,
      "grad_norm": 1.7492997646331787,
      "learning_rate": 0.0007184776292852992,
      "loss": 3.5688,
      "step": 970
    },
    {
      "epoch": 0.28471818710052293,
      "grad_norm": 1.5041091442108154,
      "learning_rate": 0.0007155723416618245,
      "loss": 3.2898,
      "step": 980
    },
    {
      "epoch": 0.2876234747239977,
      "grad_norm": 1.3806406259536743,
      "learning_rate": 0.0007126670540383499,
      "loss": 3.3282,
      "step": 990
    },
    {
      "epoch": 0.2905287623474724,
      "grad_norm": 1.3926256895065308,
      "learning_rate": 0.0007097617664148751,
      "loss": 3.355,
      "step": 1000
    },
    {
      "epoch": 0.29343404997094713,
      "grad_norm": 1.3982172012329102,
      "learning_rate": 0.0007068564787914004,
      "loss": 3.1998,
      "step": 1010
    },
    {
      "epoch": 0.2963393375944218,
      "grad_norm": 1.4983934164047241,
      "learning_rate": 0.0007039511911679256,
      "loss": 3.2165,
      "step": 1020
    },
    {
      "epoch": 0.2992446252178966,
      "grad_norm": 1.7485105991363525,
      "learning_rate": 0.0007010459035444509,
      "loss": 3.1919,
      "step": 1030
    },
    {
      "epoch": 0.30214991284137127,
      "grad_norm": 1.6209784746170044,
      "learning_rate": 0.0006981406159209762,
      "loss": 3.3874,
      "step": 1040
    },
    {
      "epoch": 0.305055200464846,
      "grad_norm": 2.420012950897217,
      "learning_rate": 0.0006952353282975015,
      "loss": 3.2686,
      "step": 1050
    },
    {
      "epoch": 0.3079604880883207,
      "grad_norm": 2.4070537090301514,
      "learning_rate": 0.0006923300406740268,
      "loss": 3.1065,
      "step": 1060
    },
    {
      "epoch": 0.31086577571179547,
      "grad_norm": 1.5184677839279175,
      "learning_rate": 0.0006894247530505519,
      "loss": 3.2381,
      "step": 1070
    },
    {
      "epoch": 0.3137710633352702,
      "grad_norm": 1.4276264905929565,
      "learning_rate": 0.0006865194654270773,
      "loss": 3.3719,
      "step": 1080
    },
    {
      "epoch": 0.3166763509587449,
      "grad_norm": 1.4421076774597168,
      "learning_rate": 0.0006836141778036026,
      "loss": 3.1814,
      "step": 1090
    },
    {
      "epoch": 0.31958163858221966,
      "grad_norm": 1.6743327379226685,
      "learning_rate": 0.0006807088901801278,
      "loss": 3.3948,
      "step": 1100
    },
    {
      "epoch": 0.32248692620569436,
      "grad_norm": 1.4972233772277832,
      "learning_rate": 0.000677803602556653,
      "loss": 3.1621,
      "step": 1110
    },
    {
      "epoch": 0.3253922138291691,
      "grad_norm": 3.121135711669922,
      "learning_rate": 0.0006748983149331784,
      "loss": 3.299,
      "step": 1120
    },
    {
      "epoch": 0.3282975014526438,
      "grad_norm": 1.4302960634231567,
      "learning_rate": 0.0006719930273097037,
      "loss": 3.4386,
      "step": 1130
    },
    {
      "epoch": 0.33120278907611855,
      "grad_norm": 1.4949315786361694,
      "learning_rate": 0.0006690877396862289,
      "loss": 3.3746,
      "step": 1140
    },
    {
      "epoch": 0.33410807669959325,
      "grad_norm": 1.643367886543274,
      "learning_rate": 0.0006661824520627542,
      "loss": 3.3791,
      "step": 1150
    },
    {
      "epoch": 0.337013364323068,
      "grad_norm": 2.0957696437835693,
      "learning_rate": 0.0006632771644392796,
      "loss": 3.2172,
      "step": 1160
    },
    {
      "epoch": 0.3399186519465427,
      "grad_norm": 1.9858819246292114,
      "learning_rate": 0.0006603718768158048,
      "loss": 3.4444,
      "step": 1170
    },
    {
      "epoch": 0.34282393957001744,
      "grad_norm": 2.6782376766204834,
      "learning_rate": 0.00065746658919233,
      "loss": 3.2084,
      "step": 1180
    },
    {
      "epoch": 0.34572922719349214,
      "grad_norm": 1.6006962060928345,
      "learning_rate": 0.0006545613015688553,
      "loss": 3.4269,
      "step": 1190
    },
    {
      "epoch": 0.3486345148169669,
      "grad_norm": 1.9934415817260742,
      "learning_rate": 0.0006516560139453807,
      "loss": 3.1264,
      "step": 1200
    },
    {
      "epoch": 0.3515398024404416,
      "grad_norm": 1.6283273696899414,
      "learning_rate": 0.0006487507263219059,
      "loss": 3.2152,
      "step": 1210
    },
    {
      "epoch": 0.35444509006391633,
      "grad_norm": 1.6118942499160767,
      "learning_rate": 0.0006458454386984311,
      "loss": 3.0089,
      "step": 1220
    },
    {
      "epoch": 0.35735037768739103,
      "grad_norm": 1.3127728700637817,
      "learning_rate": 0.0006429401510749564,
      "loss": 3.172,
      "step": 1230
    },
    {
      "epoch": 0.3602556653108658,
      "grad_norm": 1.7111369371414185,
      "learning_rate": 0.0006400348634514818,
      "loss": 3.5988,
      "step": 1240
    },
    {
      "epoch": 0.3631609529343405,
      "grad_norm": 1.6626689434051514,
      "learning_rate": 0.000637129575828007,
      "loss": 3.3172,
      "step": 1250
    },
    {
      "epoch": 0.3660662405578152,
      "grad_norm": 1.7789647579193115,
      "learning_rate": 0.0006342242882045323,
      "loss": 3.4523,
      "step": 1260
    },
    {
      "epoch": 0.3689715281812899,
      "grad_norm": 1.7544163465499878,
      "learning_rate": 0.0006313190005810575,
      "loss": 3.2473,
      "step": 1270
    },
    {
      "epoch": 0.37187681580476467,
      "grad_norm": 2.193983316421509,
      "learning_rate": 0.0006284137129575828,
      "loss": 3.1624,
      "step": 1280
    },
    {
      "epoch": 0.3747821034282394,
      "grad_norm": 2.4674675464630127,
      "learning_rate": 0.0006255084253341081,
      "loss": 3.2025,
      "step": 1290
    },
    {
      "epoch": 0.3776873910517141,
      "grad_norm": 1.4390565156936646,
      "learning_rate": 0.0006226031377106334,
      "loss": 3.4538,
      "step": 1300
    },
    {
      "epoch": 0.38059267867518887,
      "grad_norm": 1.581404447555542,
      "learning_rate": 0.0006196978500871587,
      "loss": 3.1213,
      "step": 1310
    },
    {
      "epoch": 0.38349796629866356,
      "grad_norm": 1.7093428373336792,
      "learning_rate": 0.0006167925624636838,
      "loss": 3.2353,
      "step": 1320
    },
    {
      "epoch": 0.3864032539221383,
      "grad_norm": 1.9662615060806274,
      "learning_rate": 0.0006138872748402092,
      "loss": 3.4714,
      "step": 1330
    },
    {
      "epoch": 0.389308541545613,
      "grad_norm": 4.39953088760376,
      "learning_rate": 0.0006109819872167345,
      "loss": 3.1777,
      "step": 1340
    },
    {
      "epoch": 0.39221382916908776,
      "grad_norm": 1.856909155845642,
      "learning_rate": 0.0006080766995932598,
      "loss": 3.4434,
      "step": 1350
    },
    {
      "epoch": 0.39511911679256245,
      "grad_norm": 2.3744590282440186,
      "learning_rate": 0.0006051714119697849,
      "loss": 3.1552,
      "step": 1360
    },
    {
      "epoch": 0.3980244044160372,
      "grad_norm": 1.8044947385787964,
      "learning_rate": 0.0006022661243463103,
      "loss": 3.4926,
      "step": 1370
    },
    {
      "epoch": 0.4009296920395119,
      "grad_norm": 2.4682908058166504,
      "learning_rate": 0.0005993608367228356,
      "loss": 3.3248,
      "step": 1380
    },
    {
      "epoch": 0.40383497966298665,
      "grad_norm": 2.2371652126312256,
      "learning_rate": 0.0005964555490993609,
      "loss": 3.2467,
      "step": 1390
    },
    {
      "epoch": 0.40674026728646134,
      "grad_norm": 1.452101469039917,
      "learning_rate": 0.0005935502614758861,
      "loss": 3.1716,
      "step": 1400
    },
    {
      "epoch": 0.4096455549099361,
      "grad_norm": 3.04295015335083,
      "learning_rate": 0.0005906449738524115,
      "loss": 3.086,
      "step": 1410
    },
    {
      "epoch": 0.4125508425334108,
      "grad_norm": 1.7513734102249146,
      "learning_rate": 0.0005877396862289367,
      "loss": 3.2537,
      "step": 1420
    },
    {
      "epoch": 0.41545613015688554,
      "grad_norm": 2.2942121028900146,
      "learning_rate": 0.0005848343986054619,
      "loss": 3.2261,
      "step": 1430
    },
    {
      "epoch": 0.41836141778036023,
      "grad_norm": 1.42900550365448,
      "learning_rate": 0.0005819291109819872,
      "loss": 3.1095,
      "step": 1440
    },
    {
      "epoch": 0.421266705403835,
      "grad_norm": 1.6609746217727661,
      "learning_rate": 0.0005790238233585126,
      "loss": 3.3094,
      "step": 1450
    },
    {
      "epoch": 0.4241719930273097,
      "grad_norm": 1.6042115688323975,
      "learning_rate": 0.0005761185357350379,
      "loss": 3.2376,
      "step": 1460
    },
    {
      "epoch": 0.42707728065078443,
      "grad_norm": 1.423030972480774,
      "learning_rate": 0.000573213248111563,
      "loss": 3.3406,
      "step": 1470
    },
    {
      "epoch": 0.4299825682742591,
      "grad_norm": 1.5995123386383057,
      "learning_rate": 0.0005703079604880883,
      "loss": 3.2815,
      "step": 1480
    },
    {
      "epoch": 0.4328878558977339,
      "grad_norm": 2.470207929611206,
      "learning_rate": 0.0005674026728646136,
      "loss": 3.2938,
      "step": 1490
    },
    {
      "epoch": 0.4357931435212086,
      "grad_norm": 2.2752411365509033,
      "learning_rate": 0.000564497385241139,
      "loss": 3.552,
      "step": 1500
    },
    {
      "epoch": 0.4386984311446833,
      "grad_norm": 1.9262288808822632,
      "learning_rate": 0.0005615920976176641,
      "loss": 3.2174,
      "step": 1510
    },
    {
      "epoch": 0.44160371876815807,
      "grad_norm": 1.9329161643981934,
      "learning_rate": 0.0005586868099941894,
      "loss": 3.2899,
      "step": 1520
    },
    {
      "epoch": 0.44450900639163277,
      "grad_norm": 1.8028817176818848,
      "learning_rate": 0.0005557815223707147,
      "loss": 2.992,
      "step": 1530
    },
    {
      "epoch": 0.4474142940151075,
      "grad_norm": 1.9088317155838013,
      "learning_rate": 0.00055287623474724,
      "loss": 3.1812,
      "step": 1540
    },
    {
      "epoch": 0.4503195816385822,
      "grad_norm": 1.5246951580047607,
      "learning_rate": 0.0005499709471237653,
      "loss": 3.2059,
      "step": 1550
    },
    {
      "epoch": 0.45322486926205696,
      "grad_norm": 1.6912970542907715,
      "learning_rate": 0.0005470656595002905,
      "loss": 3.0737,
      "step": 1560
    },
    {
      "epoch": 0.45613015688553166,
      "grad_norm": 1.735007882118225,
      "learning_rate": 0.0005441603718768158,
      "loss": 3.0794,
      "step": 1570
    },
    {
      "epoch": 0.4590354445090064,
      "grad_norm": 1.8051787614822388,
      "learning_rate": 0.0005412550842533411,
      "loss": 3.2102,
      "step": 1580
    },
    {
      "epoch": 0.4619407321324811,
      "grad_norm": 1.7110934257507324,
      "learning_rate": 0.0005383497966298664,
      "loss": 3.1944,
      "step": 1590
    },
    {
      "epoch": 0.46484601975595585,
      "grad_norm": 1.9159473180770874,
      "learning_rate": 0.0005354445090063917,
      "loss": 3.3614,
      "step": 1600
    },
    {
      "epoch": 0.46775130737943055,
      "grad_norm": 2.13638973236084,
      "learning_rate": 0.0005325392213829168,
      "loss": 3.3835,
      "step": 1610
    },
    {
      "epoch": 0.4706565950029053,
      "grad_norm": 1.5296388864517212,
      "learning_rate": 0.0005296339337594422,
      "loss": 3.1915,
      "step": 1620
    },
    {
      "epoch": 0.47356188262638,
      "grad_norm": 2.5676333904266357,
      "learning_rate": 0.0005267286461359675,
      "loss": 3.1709,
      "step": 1630
    },
    {
      "epoch": 0.47646717024985474,
      "grad_norm": 1.4961891174316406,
      "learning_rate": 0.0005238233585124928,
      "loss": 3.2562,
      "step": 1640
    },
    {
      "epoch": 0.47937245787332944,
      "grad_norm": 2.0515267848968506,
      "learning_rate": 0.000520918070889018,
      "loss": 3.3002,
      "step": 1650
    },
    {
      "epoch": 0.4822777454968042,
      "grad_norm": 2.223451614379883,
      "learning_rate": 0.0005180127832655433,
      "loss": 3.3226,
      "step": 1660
    },
    {
      "epoch": 0.4851830331202789,
      "grad_norm": 1.7310552597045898,
      "learning_rate": 0.0005151074956420686,
      "loss": 3.2801,
      "step": 1670
    },
    {
      "epoch": 0.48808832074375363,
      "grad_norm": 1.8511006832122803,
      "learning_rate": 0.0005122022080185939,
      "loss": 3.1074,
      "step": 1680
    },
    {
      "epoch": 0.49099360836722833,
      "grad_norm": 1.5069924592971802,
      "learning_rate": 0.0005092969203951191,
      "loss": 3.3194,
      "step": 1690
    },
    {
      "epoch": 0.4938988959907031,
      "grad_norm": 1.6078954935073853,
      "learning_rate": 0.0005063916327716444,
      "loss": 3.3405,
      "step": 1700
    },
    {
      "epoch": 0.49680418361417783,
      "grad_norm": 1.6131031513214111,
      "learning_rate": 0.0005034863451481697,
      "loss": 3.1512,
      "step": 1710
    },
    {
      "epoch": 0.4997094712376525,
      "grad_norm": 1.7539476156234741,
      "learning_rate": 0.0005005810575246949,
      "loss": 2.9103,
      "step": 1720
    },
    {
      "epoch": 0.5026147588611273,
      "grad_norm": 2.5397167205810547,
      "learning_rate": 0.0004976757699012202,
      "loss": 3.3556,
      "step": 1730
    },
    {
      "epoch": 0.505520046484602,
      "grad_norm": 1.8636236190795898,
      "learning_rate": 0.0004947704822777455,
      "loss": 3.1346,
      "step": 1740
    },
    {
      "epoch": 0.5084253341080767,
      "grad_norm": 2.0198276042938232,
      "learning_rate": 0.0004918651946542708,
      "loss": 3.4048,
      "step": 1750
    },
    {
      "epoch": 0.5113306217315514,
      "grad_norm": 1.6682415008544922,
      "learning_rate": 0.000488959907030796,
      "loss": 3.048,
      "step": 1760
    },
    {
      "epoch": 0.5142359093550262,
      "grad_norm": 1.6862215995788574,
      "learning_rate": 0.0004860546194073213,
      "loss": 3.3072,
      "step": 1770
    },
    {
      "epoch": 0.5171411969785009,
      "grad_norm": 2.2870914936065674,
      "learning_rate": 0.00048314933178384664,
      "loss": 3.1884,
      "step": 1780
    },
    {
      "epoch": 0.5200464846019756,
      "grad_norm": 1.7319608926773071,
      "learning_rate": 0.00048024404416037187,
      "loss": 3.3064,
      "step": 1790
    },
    {
      "epoch": 0.5229517722254503,
      "grad_norm": 1.5673558712005615,
      "learning_rate": 0.0004773387565368972,
      "loss": 3.2895,
      "step": 1800
    },
    {
      "epoch": 0.5258570598489251,
      "grad_norm": 1.7155338525772095,
      "learning_rate": 0.00047443346891342243,
      "loss": 3.1411,
      "step": 1810
    },
    {
      "epoch": 0.5287623474723998,
      "grad_norm": 1.6244077682495117,
      "learning_rate": 0.0004715281812899477,
      "loss": 3.3742,
      "step": 1820
    },
    {
      "epoch": 0.5316676350958744,
      "grad_norm": 1.5562231540679932,
      "learning_rate": 0.000468622893666473,
      "loss": 3.4281,
      "step": 1830
    },
    {
      "epoch": 0.5345729227193492,
      "grad_norm": 1.7228326797485352,
      "learning_rate": 0.00046571760604299827,
      "loss": 3.2709,
      "step": 1840
    },
    {
      "epoch": 0.537478210342824,
      "grad_norm": 2.2061049938201904,
      "learning_rate": 0.00046281231841952355,
      "loss": 3.1821,
      "step": 1850
    },
    {
      "epoch": 0.5403834979662987,
      "grad_norm": 1.3597869873046875,
      "learning_rate": 0.00045990703079604883,
      "loss": 3.1608,
      "step": 1860
    },
    {
      "epoch": 0.5432887855897733,
      "grad_norm": 2.0122909545898438,
      "learning_rate": 0.0004570017431725741,
      "loss": 3.2132,
      "step": 1870
    },
    {
      "epoch": 0.5461940732132481,
      "grad_norm": 1.4940990209579468,
      "learning_rate": 0.0004540964555490994,
      "loss": 3.1781,
      "step": 1880
    },
    {
      "epoch": 0.5490993608367228,
      "grad_norm": 2.3834943771362305,
      "learning_rate": 0.00045119116792562467,
      "loss": 3.2825,
      "step": 1890
    },
    {
      "epoch": 0.5520046484601976,
      "grad_norm": 1.5704680681228638,
      "learning_rate": 0.0004482858803021499,
      "loss": 3.2433,
      "step": 1900
    },
    {
      "epoch": 0.5549099360836722,
      "grad_norm": 1.4220573902130127,
      "learning_rate": 0.0004453805926786752,
      "loss": 3.4593,
      "step": 1910
    },
    {
      "epoch": 0.557815223707147,
      "grad_norm": 1.915450096130371,
      "learning_rate": 0.00044247530505520046,
      "loss": 3.216,
      "step": 1920
    },
    {
      "epoch": 0.5607205113306217,
      "grad_norm": 1.5301315784454346,
      "learning_rate": 0.00043957001743172573,
      "loss": 3.3153,
      "step": 1930
    },
    {
      "epoch": 0.5636257989540965,
      "grad_norm": 1.8796390295028687,
      "learning_rate": 0.000436664729808251,
      "loss": 3.081,
      "step": 1940
    },
    {
      "epoch": 0.5665310865775712,
      "grad_norm": 1.513083815574646,
      "learning_rate": 0.0004337594421847763,
      "loss": 3.3719,
      "step": 1950
    },
    {
      "epoch": 0.5694363742010459,
      "grad_norm": 1.87494695186615,
      "learning_rate": 0.0004308541545613016,
      "loss": 3.1842,
      "step": 1960
    },
    {
      "epoch": 0.5723416618245206,
      "grad_norm": 1.3511847257614136,
      "learning_rate": 0.00042794886693782685,
      "loss": 3.2621,
      "step": 1970
    },
    {
      "epoch": 0.5752469494479954,
      "grad_norm": 2.205536127090454,
      "learning_rate": 0.00042504357931435213,
      "loss": 3.3166,
      "step": 1980
    },
    {
      "epoch": 0.5781522370714701,
      "grad_norm": 1.4892319440841675,
      "learning_rate": 0.0004221382916908774,
      "loss": 3.3213,
      "step": 1990
    },
    {
      "epoch": 0.5810575246949448,
      "grad_norm": 1.6963030099868774,
      "learning_rate": 0.0004192330040674027,
      "loss": 3.2306,
      "step": 2000
    },
    {
      "epoch": 0.5839628123184195,
      "grad_norm": 1.550446629524231,
      "learning_rate": 0.000416327716443928,
      "loss": 3.0681,
      "step": 2010
    },
    {
      "epoch": 0.5868680999418943,
      "grad_norm": 1.8420171737670898,
      "learning_rate": 0.0004134224288204532,
      "loss": 3.2742,
      "step": 2020
    },
    {
      "epoch": 0.589773387565369,
      "grad_norm": 2.4334685802459717,
      "learning_rate": 0.00041051714119697853,
      "loss": 3.0845,
      "step": 2030
    },
    {
      "epoch": 0.5926786751888437,
      "grad_norm": 1.607600450515747,
      "learning_rate": 0.00040761185357350376,
      "loss": 3.1453,
      "step": 2040
    },
    {
      "epoch": 0.5955839628123184,
      "grad_norm": 2.1628575325012207,
      "learning_rate": 0.0004047065659500291,
      "loss": 3.2773,
      "step": 2050
    },
    {
      "epoch": 0.5984892504357932,
      "grad_norm": 1.8580620288848877,
      "learning_rate": 0.0004018012783265543,
      "loss": 3.1469,
      "step": 2060
    },
    {
      "epoch": 0.6013945380592679,
      "grad_norm": 1.7214930057525635,
      "learning_rate": 0.00039889599070307965,
      "loss": 3.1866,
      "step": 2070
    },
    {
      "epoch": 0.6042998256827425,
      "grad_norm": 2.058058977127075,
      "learning_rate": 0.0003959907030796049,
      "loss": 3.1959,
      "step": 2080
    },
    {
      "epoch": 0.6072051133062173,
      "grad_norm": 1.6369590759277344,
      "learning_rate": 0.0003930854154561302,
      "loss": 3.3515,
      "step": 2090
    },
    {
      "epoch": 0.610110400929692,
      "grad_norm": 3.0342214107513428,
      "learning_rate": 0.00039018012783265544,
      "loss": 3.199,
      "step": 2100
    },
    {
      "epoch": 0.6130156885531668,
      "grad_norm": 1.7320951223373413,
      "learning_rate": 0.00038727484020918066,
      "loss": 3.1596,
      "step": 2110
    },
    {
      "epoch": 0.6159209761766414,
      "grad_norm": 1.5654703378677368,
      "learning_rate": 0.000384369552585706,
      "loss": 3.2378,
      "step": 2120
    },
    {
      "epoch": 0.6188262638001162,
      "grad_norm": 1.6275274753570557,
      "learning_rate": 0.0003814642649622312,
      "loss": 3.2721,
      "step": 2130
    },
    {
      "epoch": 0.6217315514235909,
      "grad_norm": 1.8815613985061646,
      "learning_rate": 0.00037855897733875656,
      "loss": 3.2488,
      "step": 2140
    },
    {
      "epoch": 0.6246368390470657,
      "grad_norm": 1.553981900215149,
      "learning_rate": 0.0003756536897152818,
      "loss": 3.1466,
      "step": 2150
    },
    {
      "epoch": 0.6275421266705404,
      "grad_norm": 1.4037765264511108,
      "learning_rate": 0.0003727484020918071,
      "loss": 3.089,
      "step": 2160
    },
    {
      "epoch": 0.6304474142940151,
      "grad_norm": 1.6200047731399536,
      "learning_rate": 0.00036984311446833234,
      "loss": 3.3529,
      "step": 2170
    },
    {
      "epoch": 0.6333527019174898,
      "grad_norm": 1.437174677848816,
      "learning_rate": 0.0003669378268448577,
      "loss": 3.1684,
      "step": 2180
    },
    {
      "epoch": 0.6362579895409646,
      "grad_norm": 1.4194254875183105,
      "learning_rate": 0.0003640325392213829,
      "loss": 3.2332,
      "step": 2190
    },
    {
      "epoch": 0.6391632771644393,
      "grad_norm": 1.6270737648010254,
      "learning_rate": 0.00036112725159790824,
      "loss": 3.2119,
      "step": 2200
    },
    {
      "epoch": 0.642068564787914,
      "grad_norm": 1.7075128555297852,
      "learning_rate": 0.00035822196397443346,
      "loss": 3.2564,
      "step": 2210
    },
    {
      "epoch": 0.6449738524113887,
      "grad_norm": 2.4540176391601562,
      "learning_rate": 0.00035531667635095874,
      "loss": 3.2452,
      "step": 2220
    },
    {
      "epoch": 0.6478791400348635,
      "grad_norm": 2.213038444519043,
      "learning_rate": 0.000352411388727484,
      "loss": 3.2676,
      "step": 2230
    },
    {
      "epoch": 0.6507844276583382,
      "grad_norm": 2.007749557495117,
      "learning_rate": 0.0003495061011040093,
      "loss": 3.2527,
      "step": 2240
    },
    {
      "epoch": 0.6536897152818129,
      "grad_norm": 2.500493288040161,
      "learning_rate": 0.0003466008134805346,
      "loss": 3.1873,
      "step": 2250
    },
    {
      "epoch": 0.6565950029052876,
      "grad_norm": 1.974449872970581,
      "learning_rate": 0.00034369552585705986,
      "loss": 3.4169,
      "step": 2260
    },
    {
      "epoch": 0.6595002905287624,
      "grad_norm": 1.5874183177947998,
      "learning_rate": 0.00034079023823358514,
      "loss": 2.9968,
      "step": 2270
    },
    {
      "epoch": 0.6624055781522371,
      "grad_norm": 3.160938262939453,
      "learning_rate": 0.0003378849506101104,
      "loss": 3.1973,
      "step": 2280
    },
    {
      "epoch": 0.6653108657757117,
      "grad_norm": 2.081780433654785,
      "learning_rate": 0.0003349796629866357,
      "loss": 3.0856,
      "step": 2290
    },
    {
      "epoch": 0.6682161533991865,
      "grad_norm": 1.669944405555725,
      "learning_rate": 0.000332074375363161,
      "loss": 3.0937,
      "step": 2300
    },
    {
      "epoch": 0.6711214410226612,
      "grad_norm": 2.2438161373138428,
      "learning_rate": 0.0003291690877396862,
      "loss": 3.1705,
      "step": 2310
    },
    {
      "epoch": 0.674026728646136,
      "grad_norm": 1.6309916973114014,
      "learning_rate": 0.0003262638001162115,
      "loss": 3.1025,
      "step": 2320
    },
    {
      "epoch": 0.6769320162696106,
      "grad_norm": 1.5923348665237427,
      "learning_rate": 0.00032335851249273677,
      "loss": 3.0851,
      "step": 2330
    },
    {
      "epoch": 0.6798373038930854,
      "grad_norm": 1.7954001426696777,
      "learning_rate": 0.00032045322486926205,
      "loss": 3.2672,
      "step": 2340
    },
    {
      "epoch": 0.6827425915165601,
      "grad_norm": 1.4679991006851196,
      "learning_rate": 0.00031754793724578733,
      "loss": 3.2814,
      "step": 2350
    },
    {
      "epoch": 0.6856478791400349,
      "grad_norm": 1.5670050382614136,
      "learning_rate": 0.0003146426496223126,
      "loss": 3.4225,
      "step": 2360
    },
    {
      "epoch": 0.6885531667635096,
      "grad_norm": 1.6842602491378784,
      "learning_rate": 0.0003117373619988379,
      "loss": 3.2567,
      "step": 2370
    },
    {
      "epoch": 0.6914584543869843,
      "grad_norm": 1.3405309915542603,
      "learning_rate": 0.00030883207437536317,
      "loss": 2.9765,
      "step": 2380
    },
    {
      "epoch": 0.694363742010459,
      "grad_norm": 1.5480974912643433,
      "learning_rate": 0.00030592678675188845,
      "loss": 3.4196,
      "step": 2390
    },
    {
      "epoch": 0.6972690296339338,
      "grad_norm": 2.120706558227539,
      "learning_rate": 0.00030302149912841373,
      "loss": 3.0937,
      "step": 2400
    },
    {
      "epoch": 0.7001743172574085,
      "grad_norm": 2.0528721809387207,
      "learning_rate": 0.000300116211504939,
      "loss": 3.3134,
      "step": 2410
    },
    {
      "epoch": 0.7030796048808832,
      "grad_norm": 1.5841269493103027,
      "learning_rate": 0.00029721092388146423,
      "loss": 3.1347,
      "step": 2420
    },
    {
      "epoch": 0.7059848925043579,
      "grad_norm": 2.2423553466796875,
      "learning_rate": 0.00029430563625798957,
      "loss": 3.2368,
      "step": 2430
    },
    {
      "epoch": 0.7088901801278327,
      "grad_norm": 2.2620038986206055,
      "learning_rate": 0.0002914003486345148,
      "loss": 3.1894,
      "step": 2440
    },
    {
      "epoch": 0.7117954677513074,
      "grad_norm": 1.5946447849273682,
      "learning_rate": 0.00028849506101104013,
      "loss": 3.3605,
      "step": 2450
    },
    {
      "epoch": 0.7147007553747821,
      "grad_norm": 2.0004465579986572,
      "learning_rate": 0.00028558977338756535,
      "loss": 3.2996,
      "step": 2460
    },
    {
      "epoch": 0.7176060429982568,
      "grad_norm": 1.7948939800262451,
      "learning_rate": 0.0002826844857640907,
      "loss": 3.2978,
      "step": 2470
    },
    {
      "epoch": 0.7205113306217316,
      "grad_norm": 2.3983731269836426,
      "learning_rate": 0.0002797791981406159,
      "loss": 3.1025,
      "step": 2480
    },
    {
      "epoch": 0.7234166182452063,
      "grad_norm": 1.4078562259674072,
      "learning_rate": 0.00027687391051714125,
      "loss": 3.0136,
      "step": 2490
    },
    {
      "epoch": 0.726321905868681,
      "grad_norm": 2.151021718978882,
      "learning_rate": 0.0002739686228936665,
      "loss": 3.216,
      "step": 2500
    },
    {
      "epoch": 0.7292271934921557,
      "grad_norm": 1.5337941646575928,
      "learning_rate": 0.00027106333527019175,
      "loss": 3.2765,
      "step": 2510
    },
    {
      "epoch": 0.7321324811156305,
      "grad_norm": 1.6205577850341797,
      "learning_rate": 0.00026815804764671703,
      "loss": 3.1266,
      "step": 2520
    },
    {
      "epoch": 0.7350377687391052,
      "grad_norm": 2.116260290145874,
      "learning_rate": 0.00026525276002324226,
      "loss": 3.2008,
      "step": 2530
    },
    {
      "epoch": 0.7379430563625798,
      "grad_norm": 2.898912191390991,
      "learning_rate": 0.0002623474723997676,
      "loss": 3.4568,
      "step": 2540
    },
    {
      "epoch": 0.7408483439860546,
      "grad_norm": 1.532638430595398,
      "learning_rate": 0.0002594421847762928,
      "loss": 3.2407,
      "step": 2550
    },
    {
      "epoch": 0.7437536316095293,
      "grad_norm": 1.6406556367874146,
      "learning_rate": 0.00025653689715281815,
      "loss": 3.1213,
      "step": 2560
    },
    {
      "epoch": 0.7466589192330041,
      "grad_norm": 1.840685486793518,
      "learning_rate": 0.0002536316095293434,
      "loss": 3.2442,
      "step": 2570
    },
    {
      "epoch": 0.7495642068564788,
      "grad_norm": 1.5091896057128906,
      "learning_rate": 0.0002507263219058687,
      "loss": 3.2894,
      "step": 2580
    },
    {
      "epoch": 0.7524694944799535,
      "grad_norm": 1.6460589170455933,
      "learning_rate": 0.000247821034282394,
      "loss": 3.1626,
      "step": 2590
    },
    {
      "epoch": 0.7553747821034282,
      "grad_norm": 1.6500080823898315,
      "learning_rate": 0.00024491574665891927,
      "loss": 3.082,
      "step": 2600
    },
    {
      "epoch": 0.758280069726903,
      "grad_norm": 2.0528838634490967,
      "learning_rate": 0.00024201045903544452,
      "loss": 3.2238,
      "step": 2610
    },
    {
      "epoch": 0.7611853573503777,
      "grad_norm": 2.2341670989990234,
      "learning_rate": 0.0002391051714119698,
      "loss": 3.0818,
      "step": 2620
    },
    {
      "epoch": 0.7640906449738524,
      "grad_norm": 1.9542781114578247,
      "learning_rate": 0.00023619988378849508,
      "loss": 3.1672,
      "step": 2630
    },
    {
      "epoch": 0.7669959325973271,
      "grad_norm": 1.7119724750518799,
      "learning_rate": 0.00023329459616502034,
      "loss": 3.0936,
      "step": 2640
    },
    {
      "epoch": 0.7699012202208019,
      "grad_norm": 1.4762264490127563,
      "learning_rate": 0.00023038930854154562,
      "loss": 3.1236,
      "step": 2650
    },
    {
      "epoch": 0.7728065078442766,
      "grad_norm": 2.166940450668335,
      "learning_rate": 0.00022748402091807087,
      "loss": 3.0855,
      "step": 2660
    },
    {
      "epoch": 0.7757117954677513,
      "grad_norm": 2.9882559776306152,
      "learning_rate": 0.00022457873329459615,
      "loss": 3.1899,
      "step": 2670
    },
    {
      "epoch": 0.778617083091226,
      "grad_norm": 2.255958080291748,
      "learning_rate": 0.00022167344567112143,
      "loss": 2.9116,
      "step": 2680
    },
    {
      "epoch": 0.7815223707147008,
      "grad_norm": 1.7672749757766724,
      "learning_rate": 0.0002187681580476467,
      "loss": 3.3897,
      "step": 2690
    },
    {
      "epoch": 0.7844276583381755,
      "grad_norm": 3.2161169052124023,
      "learning_rate": 0.000215862870424172,
      "loss": 3.3202,
      "step": 2700
    },
    {
      "epoch": 0.7873329459616502,
      "grad_norm": 2.931584596633911,
      "learning_rate": 0.00021295758280069727,
      "loss": 3.0044,
      "step": 2710
    },
    {
      "epoch": 0.7902382335851249,
      "grad_norm": 3.3485841751098633,
      "learning_rate": 0.00021005229517722255,
      "loss": 3.1314,
      "step": 2720
    },
    {
      "epoch": 0.7931435212085997,
      "grad_norm": 2.6297767162323,
      "learning_rate": 0.00020714700755374783,
      "loss": 3.1446,
      "step": 2730
    },
    {
      "epoch": 0.7960488088320744,
      "grad_norm": 1.7848420143127441,
      "learning_rate": 0.0002042417199302731,
      "loss": 3.1951,
      "step": 2740
    },
    {
      "epoch": 0.798954096455549,
      "grad_norm": 1.9581565856933594,
      "learning_rate": 0.0002013364323067984,
      "loss": 3.2655,
      "step": 2750
    },
    {
      "epoch": 0.8018593840790238,
      "grad_norm": 2.317206382751465,
      "learning_rate": 0.00019843114468332364,
      "loss": 3.1849,
      "step": 2760
    },
    {
      "epoch": 0.8047646717024985,
      "grad_norm": 2.2128515243530273,
      "learning_rate": 0.00019552585705984892,
      "loss": 3.2908,
      "step": 2770
    },
    {
      "epoch": 0.8076699593259733,
      "grad_norm": 2.1160264015197754,
      "learning_rate": 0.0001926205694363742,
      "loss": 3.1951,
      "step": 2780
    },
    {
      "epoch": 0.810575246949448,
      "grad_norm": 2.541733980178833,
      "learning_rate": 0.00018971528181289948,
      "loss": 3.0563,
      "step": 2790
    },
    {
      "epoch": 0.8134805345729227,
      "grad_norm": 2.223494529724121,
      "learning_rate": 0.00018680999418942476,
      "loss": 3.3312,
      "step": 2800
    },
    {
      "epoch": 0.8163858221963974,
      "grad_norm": 2.880728006362915,
      "learning_rate": 0.00018390470656595004,
      "loss": 3.179,
      "step": 2810
    },
    {
      "epoch": 0.8192911098198722,
      "grad_norm": 1.741552710533142,
      "learning_rate": 0.00018099941894247532,
      "loss": 3.1733,
      "step": 2820
    },
    {
      "epoch": 0.8221963974433469,
      "grad_norm": 1.7234179973602295,
      "learning_rate": 0.0001780941313190006,
      "loss": 2.9257,
      "step": 2830
    },
    {
      "epoch": 0.8251016850668216,
      "grad_norm": 1.842891812324524,
      "learning_rate": 0.00017518884369552588,
      "loss": 3.184,
      "step": 2840
    },
    {
      "epoch": 0.8280069726902963,
      "grad_norm": 1.5466854572296143,
      "learning_rate": 0.00017228355607205113,
      "loss": 3.2429,
      "step": 2850
    },
    {
      "epoch": 0.8309122603137711,
      "grad_norm": 1.6098315715789795,
      "learning_rate": 0.0001693782684485764,
      "loss": 3.2748,
      "step": 2860
    },
    {
      "epoch": 0.8338175479372458,
      "grad_norm": 2.2846431732177734,
      "learning_rate": 0.00016647298082510167,
      "loss": 3.0502,
      "step": 2870
    },
    {
      "epoch": 0.8367228355607205,
      "grad_norm": 1.806368350982666,
      "learning_rate": 0.00016356769320162695,
      "loss": 3.0075,
      "step": 2880
    },
    {
      "epoch": 0.8396281231841952,
      "grad_norm": 2.622936487197876,
      "learning_rate": 0.00016066240557815223,
      "loss": 3.2362,
      "step": 2890
    },
    {
      "epoch": 0.84253341080767,
      "grad_norm": 1.5904768705368042,
      "learning_rate": 0.0001577571179546775,
      "loss": 3.3858,
      "step": 2900
    },
    {
      "epoch": 0.8454386984311447,
      "grad_norm": 1.5727643966674805,
      "learning_rate": 0.0001548518303312028,
      "loss": 3.1311,
      "step": 2910
    },
    {
      "epoch": 0.8483439860546194,
      "grad_norm": 1.9567508697509766,
      "learning_rate": 0.00015194654270772807,
      "loss": 3.0411,
      "step": 2920
    },
    {
      "epoch": 0.8512492736780941,
      "grad_norm": 1.7126290798187256,
      "learning_rate": 0.00014904125508425335,
      "loss": 3.1958,
      "step": 2930
    },
    {
      "epoch": 0.8541545613015689,
      "grad_norm": 1.643919587135315,
      "learning_rate": 0.00014613596746077863,
      "loss": 3.319,
      "step": 2940
    },
    {
      "epoch": 0.8570598489250436,
      "grad_norm": 1.8096864223480225,
      "learning_rate": 0.0001432306798373039,
      "loss": 3.0672,
      "step": 2950
    },
    {
      "epoch": 0.8599651365485182,
      "grad_norm": 1.626797080039978,
      "learning_rate": 0.00014032539221382916,
      "loss": 3.1448,
      "step": 2960
    },
    {
      "epoch": 0.862870424171993,
      "grad_norm": 1.8186558485031128,
      "learning_rate": 0.00013742010459035444,
      "loss": 3.1171,
      "step": 2970
    },
    {
      "epoch": 0.8657757117954678,
      "grad_norm": 1.5506033897399902,
      "learning_rate": 0.00013451481696687972,
      "loss": 3.2405,
      "step": 2980
    },
    {
      "epoch": 0.8686809994189425,
      "grad_norm": 1.5982096195220947,
      "learning_rate": 0.000131609529343405,
      "loss": 3.239,
      "step": 2990
    },
    {
      "epoch": 0.8715862870424173,
      "grad_norm": 2.753303289413452,
      "learning_rate": 0.00012870424171993028,
      "loss": 3.0688,
      "step": 3000
    },
    {
      "epoch": 0.8744915746658919,
      "grad_norm": 1.5672305822372437,
      "learning_rate": 0.00012579895409645556,
      "loss": 3.3763,
      "step": 3010
    },
    {
      "epoch": 0.8773968622893666,
      "grad_norm": 1.7478954792022705,
      "learning_rate": 0.0001228936664729808,
      "loss": 3.4566,
      "step": 3020
    },
    {
      "epoch": 0.8803021499128414,
      "grad_norm": 1.6851907968521118,
      "learning_rate": 0.0001199883788495061,
      "loss": 3.0624,
      "step": 3030
    },
    {
      "epoch": 0.8832074375363161,
      "grad_norm": 1.5399339199066162,
      "learning_rate": 0.00011708309122603137,
      "loss": 3.3421,
      "step": 3040
    },
    {
      "epoch": 0.8861127251597908,
      "grad_norm": 2.0999886989593506,
      "learning_rate": 0.00011417780360255665,
      "loss": 3.3306,
      "step": 3050
    },
    {
      "epoch": 0.8890180127832655,
      "grad_norm": 1.520297646522522,
      "learning_rate": 0.00011127251597908193,
      "loss": 3.1675,
      "step": 3060
    },
    {
      "epoch": 0.8919233004067403,
      "grad_norm": 1.5661578178405762,
      "learning_rate": 0.00010836722835560721,
      "loss": 3.2599,
      "step": 3070
    },
    {
      "epoch": 0.894828588030215,
      "grad_norm": 2.129974365234375,
      "learning_rate": 0.00010546194073213249,
      "loss": 3.2679,
      "step": 3080
    },
    {
      "epoch": 0.8977338756536897,
      "grad_norm": 2.0718746185302734,
      "learning_rate": 0.00010255665310865776,
      "loss": 2.8585,
      "step": 3090
    },
    {
      "epoch": 0.9006391632771644,
      "grad_norm": 2.6668145656585693,
      "learning_rate": 9.965136548518304e-05,
      "loss": 3.3708,
      "step": 3100
    },
    {
      "epoch": 0.9035444509006392,
      "grad_norm": 1.8130314350128174,
      "learning_rate": 9.674607786170832e-05,
      "loss": 2.9335,
      "step": 3110
    },
    {
      "epoch": 0.9064497385241139,
      "grad_norm": 2.4834818840026855,
      "learning_rate": 9.384079023823358e-05,
      "loss": 3.0529,
      "step": 3120
    },
    {
      "epoch": 0.9093550261475886,
      "grad_norm": 2.329857110977173,
      "learning_rate": 9.093550261475886e-05,
      "loss": 3.2453,
      "step": 3130
    },
    {
      "epoch": 0.9122603137710633,
      "grad_norm": 1.5561027526855469,
      "learning_rate": 8.803021499128413e-05,
      "loss": 3.3145,
      "step": 3140
    },
    {
      "epoch": 0.9151656013945381,
      "grad_norm": 2.183462619781494,
      "learning_rate": 8.512492736780941e-05,
      "loss": 3.1134,
      "step": 3150
    },
    {
      "epoch": 0.9180708890180128,
      "grad_norm": 2.460347890853882,
      "learning_rate": 8.221963974433469e-05,
      "loss": 3.1344,
      "step": 3160
    },
    {
      "epoch": 0.9209761766414875,
      "grad_norm": 1.6561336517333984,
      "learning_rate": 7.931435212085997e-05,
      "loss": 3.219,
      "step": 3170
    },
    {
      "epoch": 0.9238814642649622,
      "grad_norm": 1.737046718597412,
      "learning_rate": 7.640906449738525e-05,
      "loss": 3.1506,
      "step": 3180
    },
    {
      "epoch": 0.926786751888437,
      "grad_norm": 1.5602611303329468,
      "learning_rate": 7.350377687391052e-05,
      "loss": 3.294,
      "step": 3190
    },
    {
      "epoch": 0.9296920395119117,
      "grad_norm": 1.4723647832870483,
      "learning_rate": 7.05984892504358e-05,
      "loss": 3.4249,
      "step": 3200
    },
    {
      "epoch": 0.9325973271353865,
      "grad_norm": 1.746763825416565,
      "learning_rate": 6.769320162696108e-05,
      "loss": 3.0146,
      "step": 3210
    },
    {
      "epoch": 0.9355026147588611,
      "grad_norm": 2.3248188495635986,
      "learning_rate": 6.478791400348634e-05,
      "loss": 3.4176,
      "step": 3220
    },
    {
      "epoch": 0.9384079023823358,
      "grad_norm": 1.9288208484649658,
      "learning_rate": 6.188262638001162e-05,
      "loss": 3.3002,
      "step": 3230
    },
    {
      "epoch": 0.9413131900058106,
      "grad_norm": 1.53579580783844,
      "learning_rate": 5.89773387565369e-05,
      "loss": 3.2135,
      "step": 3240
    },
    {
      "epoch": 0.9442184776292853,
      "grad_norm": 1.6293129920959473,
      "learning_rate": 5.607205113306217e-05,
      "loss": 3.234,
      "step": 3250
    },
    {
      "epoch": 0.94712376525276,
      "grad_norm": 1.5556848049163818,
      "learning_rate": 5.316676350958745e-05,
      "loss": 3.0105,
      "step": 3260
    },
    {
      "epoch": 0.9500290528762347,
      "grad_norm": 1.8462975025177002,
      "learning_rate": 5.026147588611273e-05,
      "loss": 3.3316,
      "step": 3270
    },
    {
      "epoch": 0.9529343404997095,
      "grad_norm": 2.009291887283325,
      "learning_rate": 4.7356188262638e-05,
      "loss": 3.1963,
      "step": 3280
    },
    {
      "epoch": 0.9558396281231842,
      "grad_norm": 2.564866542816162,
      "learning_rate": 4.445090063916328e-05,
      "loss": 3.0943,
      "step": 3290
    },
    {
      "epoch": 0.9587449157466589,
      "grad_norm": 2.287389039993286,
      "learning_rate": 4.154561301568855e-05,
      "loss": 3.1489,
      "step": 3300
    },
    {
      "epoch": 0.9616502033701336,
      "grad_norm": 1.7018791437149048,
      "learning_rate": 3.864032539221383e-05,
      "loss": 3.2196,
      "step": 3310
    },
    {
      "epoch": 0.9645554909936084,
      "grad_norm": 1.5301369428634644,
      "learning_rate": 3.573503776873911e-05,
      "loss": 3.2501,
      "step": 3320
    },
    {
      "epoch": 0.9674607786170831,
      "grad_norm": 1.9756900072097778,
      "learning_rate": 3.282975014526438e-05,
      "loss": 3.3142,
      "step": 3330
    },
    {
      "epoch": 0.9703660662405578,
      "grad_norm": 2.302877187728882,
      "learning_rate": 2.9924462521789657e-05,
      "loss": 3.0929,
      "step": 3340
    },
    {
      "epoch": 0.9732713538640325,
      "grad_norm": 2.6692914962768555,
      "learning_rate": 2.7019174898314933e-05,
      "loss": 3.1879,
      "step": 3350
    },
    {
      "epoch": 0.9761766414875073,
      "grad_norm": 1.9064438343048096,
      "learning_rate": 2.411388727484021e-05,
      "loss": 3.1453,
      "step": 3360
    },
    {
      "epoch": 0.979081929110982,
      "grad_norm": 1.5330086946487427,
      "learning_rate": 2.1208599651365483e-05,
      "loss": 3.444,
      "step": 3370
    },
    {
      "epoch": 0.9819872167344567,
      "grad_norm": 1.4686428308486938,
      "learning_rate": 1.8303312027890763e-05,
      "loss": 3.2057,
      "step": 3380
    },
    {
      "epoch": 0.9848925043579314,
      "grad_norm": 1.634229302406311,
      "learning_rate": 1.5398024404416036e-05,
      "loss": 3.1672,
      "step": 3390
    },
    {
      "epoch": 0.9877977919814062,
      "grad_norm": 1.6302204132080078,
      "learning_rate": 1.2492736780941313e-05,
      "loss": 3.3837,
      "step": 3400
    },
    {
      "epoch": 0.9907030796048809,
      "grad_norm": 1.6237066984176636,
      "learning_rate": 9.58744915746659e-06,
      "loss": 3.2046,
      "step": 3410
    },
    {
      "epoch": 0.9936083672283557,
      "grad_norm": 2.8436825275421143,
      "learning_rate": 6.682161533991866e-06,
      "loss": 3.2993,
      "step": 3420
    },
    {
      "epoch": 0.9965136548518303,
      "grad_norm": 1.7318236827850342,
      "learning_rate": 3.7768739105171414e-06,
      "loss": 3.2543,
      "step": 3430
    },
    {
      "epoch": 0.999418942475305,
      "grad_norm": 2.368562936782837,
      "learning_rate": 8.715862870424173e-07,
      "loss": 3.0215,
      "step": 3440
    },
    {
      "epoch": 1.0,
      "eval_loss": 3.083611249923706,
      "eval_runtime": 217.6979,
      "eval_samples_per_second": 126.469,
      "eval_steps_per_second": 3.955,
      "step": 3442
    }
  ],
  "logging_steps": 10,
  "max_steps": 3442,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 102888733824000.0,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
